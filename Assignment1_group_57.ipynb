{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonatandn/IDL/blob/main/Assignment1_group_57.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 1. Music Century Classification\n",
        "\n",
        "**Assignment Responsible**: Natalie Lang.\n",
        "\n",
        "In this assignment, we will build models to predict which\n",
        "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
        "based on the Million Song Dataset. The data is available to download from the UCI \n",
        "Machine Learning Repository. Here are some links about the data:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Note that you are note allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## Question 1. Data (21%)\n",
        "\n",
        "Start by setting up a Google Colab notebook in which to do your work.\n",
        "Since you are working with a partner, you might find this link helpful:\n",
        "\n",
        "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "\n",
        "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together. \n",
        "\n",
        "To process and read the data, we use the popular `pandas` package for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
        "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
        "The first method is easier but slower, and the second method is a bit involved at first, but\n",
        "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
        "figuring how to do that now.\n",
        "\n",
        "Here are some resources to help you get started:\n",
        "\n",
        "- http.://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_"
      },
      "source": [
        "load_from_drive = False\n",
        "\n",
        "if not load_from_drive:\n",
        "  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  csv_path = '/content/gdrive/My Drive/YearPredictionMSD.txt.zip' # TODO - UPDATE ME WITH THE TRUE PATH!\n",
        "\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path, names=t_label + x_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bBEnj3zGq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "f4190e6e-02c5-44e0-941e-d661dc41de02"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        year      var1      var2      var3      var4      var5      var6  \\\n",
              "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "...      ...       ...       ...       ...       ...       ...       ...   \n",
              "515340  2006  51.28467  45.88068  22.19582  -5.53319  -3.61835 -16.36914   \n",
              "515341  2006  49.87870  37.93125  18.65987  -3.63581 -27.75665 -18.52988   \n",
              "515342  2006  45.12852  12.65758 -38.72018   8.80882 -29.29985  -2.28706   \n",
              "515343  2006  44.16614  32.38368  -3.34971  -2.49165 -19.59278 -18.67098   \n",
              "515344  2005  51.85726  59.11655  26.39436  -5.46030 -20.69012 -19.95528   \n",
              "\n",
              "            var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "...          ...       ...       ...  ...       ...        ...        ...   \n",
              "515340   2.12652   5.18160  -8.66890  ...   4.81440   -3.75991  -30.92584   \n",
              "515341   7.76108   3.56109  -2.50351  ...  32.38589  -32.75535  -61.05473   \n",
              "515342 -18.40424 -22.28726  -4.52429  ... -18.73598  -71.15954 -123.98443   \n",
              "515343   8.78428   4.02039 -12.01230  ...  67.16763  282.77624   -4.63677   \n",
              "515344  -6.72771   2.29590  10.31018  ... -11.50511  -69.18291   60.58456   \n",
              "\n",
              "            var84     var85     var86      var87     var88      var89  \\\n",
              "0        15.37344   1.11144 -23.08793   68.40795  -1.82223  -27.46348   \n",
              "1        42.87836  -9.90378 -32.22788   70.49388  12.04941   58.43453   \n",
              "2        10.93792  -0.07568  43.20130 -115.00698  -0.05859   39.67068   \n",
              "3       -46.67617 -12.51516  82.58061  -72.08993   9.90558  199.62971   \n",
              "4       -17.72522  -1.49237  -7.50035   51.76631   7.88713   55.66926   \n",
              "...           ...       ...       ...        ...       ...        ...   \n",
              "515340   26.33968  -5.03390  21.86037 -142.29410   3.42901  -41.14721   \n",
              "515341   56.65182  15.29965  95.88193  -10.63242  12.96552   92.11633   \n",
              "515342  121.26989  10.89629  34.62409 -248.61020  -6.07171   53.96319   \n",
              "515343  144.00125  21.62652 -29.72432   71.47198  20.32240   14.83107   \n",
              "515344   28.64599  -4.39620 -64.56491  -45.61012  -5.51512   32.35602   \n",
              "\n",
              "           var90  \n",
              "0        2.26327  \n",
              "1       26.92061  \n",
              "2       -0.66345  \n",
              "3       18.85382  \n",
              "4       28.74903  \n",
              "...          ...  \n",
              "515340 -15.46052  \n",
              "515341  10.88815  \n",
              "515342  -8.09364  \n",
              "515343  39.74909  \n",
              "515344  12.17352  \n",
              "\n",
              "[515345 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4bb5282d-35ac-42a0-9ec6-4693b5845427\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515340</th>\n",
              "      <td>2006</td>\n",
              "      <td>51.28467</td>\n",
              "      <td>45.88068</td>\n",
              "      <td>22.19582</td>\n",
              "      <td>-5.53319</td>\n",
              "      <td>-3.61835</td>\n",
              "      <td>-16.36914</td>\n",
              "      <td>2.12652</td>\n",
              "      <td>5.18160</td>\n",
              "      <td>-8.66890</td>\n",
              "      <td>...</td>\n",
              "      <td>4.81440</td>\n",
              "      <td>-3.75991</td>\n",
              "      <td>-30.92584</td>\n",
              "      <td>26.33968</td>\n",
              "      <td>-5.03390</td>\n",
              "      <td>21.86037</td>\n",
              "      <td>-142.29410</td>\n",
              "      <td>3.42901</td>\n",
              "      <td>-41.14721</td>\n",
              "      <td>-15.46052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515341</th>\n",
              "      <td>2006</td>\n",
              "      <td>49.87870</td>\n",
              "      <td>37.93125</td>\n",
              "      <td>18.65987</td>\n",
              "      <td>-3.63581</td>\n",
              "      <td>-27.75665</td>\n",
              "      <td>-18.52988</td>\n",
              "      <td>7.76108</td>\n",
              "      <td>3.56109</td>\n",
              "      <td>-2.50351</td>\n",
              "      <td>...</td>\n",
              "      <td>32.38589</td>\n",
              "      <td>-32.75535</td>\n",
              "      <td>-61.05473</td>\n",
              "      <td>56.65182</td>\n",
              "      <td>15.29965</td>\n",
              "      <td>95.88193</td>\n",
              "      <td>-10.63242</td>\n",
              "      <td>12.96552</td>\n",
              "      <td>92.11633</td>\n",
              "      <td>10.88815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515342</th>\n",
              "      <td>2006</td>\n",
              "      <td>45.12852</td>\n",
              "      <td>12.65758</td>\n",
              "      <td>-38.72018</td>\n",
              "      <td>8.80882</td>\n",
              "      <td>-29.29985</td>\n",
              "      <td>-2.28706</td>\n",
              "      <td>-18.40424</td>\n",
              "      <td>-22.28726</td>\n",
              "      <td>-4.52429</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.73598</td>\n",
              "      <td>-71.15954</td>\n",
              "      <td>-123.98443</td>\n",
              "      <td>121.26989</td>\n",
              "      <td>10.89629</td>\n",
              "      <td>34.62409</td>\n",
              "      <td>-248.61020</td>\n",
              "      <td>-6.07171</td>\n",
              "      <td>53.96319</td>\n",
              "      <td>-8.09364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515343</th>\n",
              "      <td>2006</td>\n",
              "      <td>44.16614</td>\n",
              "      <td>32.38368</td>\n",
              "      <td>-3.34971</td>\n",
              "      <td>-2.49165</td>\n",
              "      <td>-19.59278</td>\n",
              "      <td>-18.67098</td>\n",
              "      <td>8.78428</td>\n",
              "      <td>4.02039</td>\n",
              "      <td>-12.01230</td>\n",
              "      <td>...</td>\n",
              "      <td>67.16763</td>\n",
              "      <td>282.77624</td>\n",
              "      <td>-4.63677</td>\n",
              "      <td>144.00125</td>\n",
              "      <td>21.62652</td>\n",
              "      <td>-29.72432</td>\n",
              "      <td>71.47198</td>\n",
              "      <td>20.32240</td>\n",
              "      <td>14.83107</td>\n",
              "      <td>39.74909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515344</th>\n",
              "      <td>2005</td>\n",
              "      <td>51.85726</td>\n",
              "      <td>59.11655</td>\n",
              "      <td>26.39436</td>\n",
              "      <td>-5.46030</td>\n",
              "      <td>-20.69012</td>\n",
              "      <td>-19.95528</td>\n",
              "      <td>-6.72771</td>\n",
              "      <td>2.29590</td>\n",
              "      <td>10.31018</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.50511</td>\n",
              "      <td>-69.18291</td>\n",
              "      <td>60.58456</td>\n",
              "      <td>28.64599</td>\n",
              "      <td>-4.39620</td>\n",
              "      <td>-64.56491</td>\n",
              "      <td>-45.61012</td>\n",
              "      <td>-5.51512</td>\n",
              "      <td>32.35602</td>\n",
              "      <td>12.17352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>515345 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bb5282d-35ac-42a0-9ec6-4693b5845427')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4bb5282d-35ac-42a0-9ec6-4693b5845427 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4bb5282d-35ac-42a0-9ec6-4693b5845427');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugy7FZ8eoAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "outputId": "3868cad2-85ba-464f-e2f1-1da9f89d23a3"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    year      var1       var2      var3      var4      var5      var6  \\\n",
              "0      1  49.94357   21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1      1  48.73215   18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2      1  50.95714   31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3      1  48.24750   -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4      1  50.97020   42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "5      1  50.54767    0.31568  92.35066  22.38696 -25.51870 -19.04928   \n",
              "6      1  50.57546   33.17843  50.53517  11.55217 -27.24764  -8.78206   \n",
              "7      1  48.26892    8.97526  75.23158  24.04945 -16.02105 -14.09491   \n",
              "8      1  49.75468   33.99581  56.73846   2.89581  -2.92429 -26.44413   \n",
              "9      1  45.17809   46.34234 -40.65357  -2.47909   1.21253  -0.65302   \n",
              "10     1  39.13076  -23.01763 -36.20583   1.67519  -4.27101  13.01158   \n",
              "11     1  37.66498  -34.05910 -17.36060 -26.77781 -39.95119 -20.75000   \n",
              "12     1  26.51957 -148.15762 -13.30095  -7.25851  17.22029 -21.99439   \n",
              "13     1  37.68491  -26.84185 -27.10566 -14.95883  -5.87200 -21.68979   \n",
              "14     0  39.11695   -8.29767 -51.37966  -4.42668 -30.06506 -11.95916   \n",
              "15     1  35.05129  -67.97714 -14.20239  -6.68696  -0.61230 -18.70341   \n",
              "16     1  33.63129  -96.14912 -89.38216 -12.11699  13.77252  -6.69377   \n",
              "17     0  41.38639  -20.78665  51.80155  17.21415 -36.44189 -11.53169   \n",
              "18     0  37.45034   11.42615  56.28982  19.58426 -16.43530   2.22457   \n",
              "19     0  39.71092   -4.92800  12.88590 -11.87773   2.48031 -16.11028   \n",
              "\n",
              "        var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0  -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1    8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2   -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3    5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4  -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "5   20.67345  -5.19943   3.63566  ...   6.59753  -50.69577   26.02574   \n",
              "6  -12.04282  -9.53930  28.61811  ...  11.63681   25.44182  134.62382   \n",
              "7    8.11871  -1.87566   7.46701  ...  18.03989  -58.46192  -65.56438   \n",
              "8    1.71392  -0.55644  22.08594  ...  18.70812    5.20391  -27.75192   \n",
              "9   -6.95536 -12.20040  17.02512  ...  -4.36742  -87.55285  -70.79677   \n",
              "10   8.05718  -8.41088   6.27370  ...  32.86051  -26.08461 -186.82429   \n",
              "11  -0.10231  -0.89972  -1.30205  ...  11.18909   45.20614   53.83925   \n",
              "12   5.51947   3.48418   2.61738  ...  23.80442  251.76360   18.81642   \n",
              "13   4.87374 -18.01800   1.52141  ... -67.57637  234.27192  -72.34557   \n",
              "14  -0.85322  -8.86179  11.36680  ...  42.22923  478.26580  -10.33823   \n",
              "15  -1.31928  -9.46370   5.53492  ...  10.25585   94.90539   15.95689   \n",
              "16 -33.36843 -24.81437  21.22757  ...  49.93249  -14.47489   40.70590   \n",
              "17  11.75252  -7.62428  -3.65488  ...  50.37614  -40.48205   48.07805   \n",
              "18   1.02668  -7.34736  -0.01184  ... -22.46207  -25.77228 -322.42841   \n",
              "19 -16.40421  -8.29657   9.86817  ...  11.92816  -73.72412   16.19039   \n",
              "\n",
              "        var84     var85      var86      var87     var88       var89     var90  \n",
              "0    15.37344   1.11144  -23.08793   68.40795  -1.82223   -27.46348   2.26327  \n",
              "1    42.87836  -9.90378  -32.22788   70.49388  12.04941    58.43453  26.92061  \n",
              "2    10.93792  -0.07568   43.20130 -115.00698  -0.05859    39.67068  -0.66345  \n",
              "3   -46.67617 -12.51516   82.58061  -72.08993   9.90558   199.62971  18.85382  \n",
              "4   -17.72522  -1.49237   -7.50035   51.76631   7.88713    55.66926  28.74903  \n",
              "5    18.94430  -0.33730    6.09352   35.18381   5.00283   -11.02257   0.02263  \n",
              "6    21.51982   8.17570   35.46251   11.57736   4.50056    -4.62739   1.40192  \n",
              "7    46.99856  -4.09602   56.37650  -18.29975  -0.30633     3.98364  -3.72556  \n",
              "8    17.22100  -0.85210  -15.67150  -26.36257   5.48708    -9.13495   6.08680  \n",
              "9    76.57355  -7.71727    3.26926 -298.49845  11.49326   -89.21804 -15.09719  \n",
              "10  113.58176   9.28727   44.60282  158.00425  -2.59543   109.19723  23.36143  \n",
              "11    2.59467  -4.00958  -47.74886 -170.92864  -5.19009     8.83617  -7.16056  \n",
              "12  157.09656 -27.79449 -137.72740  115.28414  23.00230  -164.02536  51.54138  \n",
              "13 -362.25101 -25.55019  -89.08971 -891.58937  14.11648 -1030.99180  99.28967  \n",
              "14 -103.76858  39.19511  -98.76636 -122.81061  -2.14942  -211.48202 -12.81569  \n",
              "15  -98.15732  -9.64859  -93.52834  -95.82981  20.73063  -562.07671  43.44696  \n",
              "16   58.63692   8.81522   27.28474    5.78046   3.44539   259.10825  10.28525  \n",
              "17   -7.62399   6.51934  -30.46090  -53.87264   4.44627    58.16913  -0.02409  \n",
              "18 -146.57408  13.61588   92.22918 -439.80259  25.73235   157.22967  38.70617  \n",
              "19    9.79606   9.71693   -9.90907  -20.65851   2.34002   -31.57015   1.58400  \n",
              "\n",
              "[20 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42c6aa0d-d1ca-4462-be29-bcabf093442d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>...</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>...</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>...</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>...</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>39.13076</td>\n",
              "      <td>-23.01763</td>\n",
              "      <td>-36.20583</td>\n",
              "      <td>1.67519</td>\n",
              "      <td>-4.27101</td>\n",
              "      <td>13.01158</td>\n",
              "      <td>8.05718</td>\n",
              "      <td>-8.41088</td>\n",
              "      <td>6.27370</td>\n",
              "      <td>...</td>\n",
              "      <td>32.86051</td>\n",
              "      <td>-26.08461</td>\n",
              "      <td>-186.82429</td>\n",
              "      <td>113.58176</td>\n",
              "      <td>9.28727</td>\n",
              "      <td>44.60282</td>\n",
              "      <td>158.00425</td>\n",
              "      <td>-2.59543</td>\n",
              "      <td>109.19723</td>\n",
              "      <td>23.36143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>37.66498</td>\n",
              "      <td>-34.05910</td>\n",
              "      <td>-17.36060</td>\n",
              "      <td>-26.77781</td>\n",
              "      <td>-39.95119</td>\n",
              "      <td>-20.75000</td>\n",
              "      <td>-0.10231</td>\n",
              "      <td>-0.89972</td>\n",
              "      <td>-1.30205</td>\n",
              "      <td>...</td>\n",
              "      <td>11.18909</td>\n",
              "      <td>45.20614</td>\n",
              "      <td>53.83925</td>\n",
              "      <td>2.59467</td>\n",
              "      <td>-4.00958</td>\n",
              "      <td>-47.74886</td>\n",
              "      <td>-170.92864</td>\n",
              "      <td>-5.19009</td>\n",
              "      <td>8.83617</td>\n",
              "      <td>-7.16056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>26.51957</td>\n",
              "      <td>-148.15762</td>\n",
              "      <td>-13.30095</td>\n",
              "      <td>-7.25851</td>\n",
              "      <td>17.22029</td>\n",
              "      <td>-21.99439</td>\n",
              "      <td>5.51947</td>\n",
              "      <td>3.48418</td>\n",
              "      <td>2.61738</td>\n",
              "      <td>...</td>\n",
              "      <td>23.80442</td>\n",
              "      <td>251.76360</td>\n",
              "      <td>18.81642</td>\n",
              "      <td>157.09656</td>\n",
              "      <td>-27.79449</td>\n",
              "      <td>-137.72740</td>\n",
              "      <td>115.28414</td>\n",
              "      <td>23.00230</td>\n",
              "      <td>-164.02536</td>\n",
              "      <td>51.54138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>37.68491</td>\n",
              "      <td>-26.84185</td>\n",
              "      <td>-27.10566</td>\n",
              "      <td>-14.95883</td>\n",
              "      <td>-5.87200</td>\n",
              "      <td>-21.68979</td>\n",
              "      <td>4.87374</td>\n",
              "      <td>-18.01800</td>\n",
              "      <td>1.52141</td>\n",
              "      <td>...</td>\n",
              "      <td>-67.57637</td>\n",
              "      <td>234.27192</td>\n",
              "      <td>-72.34557</td>\n",
              "      <td>-362.25101</td>\n",
              "      <td>-25.55019</td>\n",
              "      <td>-89.08971</td>\n",
              "      <td>-891.58937</td>\n",
              "      <td>14.11648</td>\n",
              "      <td>-1030.99180</td>\n",
              "      <td>99.28967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>39.11695</td>\n",
              "      <td>-8.29767</td>\n",
              "      <td>-51.37966</td>\n",
              "      <td>-4.42668</td>\n",
              "      <td>-30.06506</td>\n",
              "      <td>-11.95916</td>\n",
              "      <td>-0.85322</td>\n",
              "      <td>-8.86179</td>\n",
              "      <td>11.36680</td>\n",
              "      <td>...</td>\n",
              "      <td>42.22923</td>\n",
              "      <td>478.26580</td>\n",
              "      <td>-10.33823</td>\n",
              "      <td>-103.76858</td>\n",
              "      <td>39.19511</td>\n",
              "      <td>-98.76636</td>\n",
              "      <td>-122.81061</td>\n",
              "      <td>-2.14942</td>\n",
              "      <td>-211.48202</td>\n",
              "      <td>-12.81569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>35.05129</td>\n",
              "      <td>-67.97714</td>\n",
              "      <td>-14.20239</td>\n",
              "      <td>-6.68696</td>\n",
              "      <td>-0.61230</td>\n",
              "      <td>-18.70341</td>\n",
              "      <td>-1.31928</td>\n",
              "      <td>-9.46370</td>\n",
              "      <td>5.53492</td>\n",
              "      <td>...</td>\n",
              "      <td>10.25585</td>\n",
              "      <td>94.90539</td>\n",
              "      <td>15.95689</td>\n",
              "      <td>-98.15732</td>\n",
              "      <td>-9.64859</td>\n",
              "      <td>-93.52834</td>\n",
              "      <td>-95.82981</td>\n",
              "      <td>20.73063</td>\n",
              "      <td>-562.07671</td>\n",
              "      <td>43.44696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>33.63129</td>\n",
              "      <td>-96.14912</td>\n",
              "      <td>-89.38216</td>\n",
              "      <td>-12.11699</td>\n",
              "      <td>13.77252</td>\n",
              "      <td>-6.69377</td>\n",
              "      <td>-33.36843</td>\n",
              "      <td>-24.81437</td>\n",
              "      <td>21.22757</td>\n",
              "      <td>...</td>\n",
              "      <td>49.93249</td>\n",
              "      <td>-14.47489</td>\n",
              "      <td>40.70590</td>\n",
              "      <td>58.63692</td>\n",
              "      <td>8.81522</td>\n",
              "      <td>27.28474</td>\n",
              "      <td>5.78046</td>\n",
              "      <td>3.44539</td>\n",
              "      <td>259.10825</td>\n",
              "      <td>10.28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>41.38639</td>\n",
              "      <td>-20.78665</td>\n",
              "      <td>51.80155</td>\n",
              "      <td>17.21415</td>\n",
              "      <td>-36.44189</td>\n",
              "      <td>-11.53169</td>\n",
              "      <td>11.75252</td>\n",
              "      <td>-7.62428</td>\n",
              "      <td>-3.65488</td>\n",
              "      <td>...</td>\n",
              "      <td>50.37614</td>\n",
              "      <td>-40.48205</td>\n",
              "      <td>48.07805</td>\n",
              "      <td>-7.62399</td>\n",
              "      <td>6.51934</td>\n",
              "      <td>-30.46090</td>\n",
              "      <td>-53.87264</td>\n",
              "      <td>4.44627</td>\n",
              "      <td>58.16913</td>\n",
              "      <td>-0.02409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>37.45034</td>\n",
              "      <td>11.42615</td>\n",
              "      <td>56.28982</td>\n",
              "      <td>19.58426</td>\n",
              "      <td>-16.43530</td>\n",
              "      <td>2.22457</td>\n",
              "      <td>1.02668</td>\n",
              "      <td>-7.34736</td>\n",
              "      <td>-0.01184</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.46207</td>\n",
              "      <td>-25.77228</td>\n",
              "      <td>-322.42841</td>\n",
              "      <td>-146.57408</td>\n",
              "      <td>13.61588</td>\n",
              "      <td>92.22918</td>\n",
              "      <td>-439.80259</td>\n",
              "      <td>25.73235</td>\n",
              "      <td>157.22967</td>\n",
              "      <td>38.70617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>39.71092</td>\n",
              "      <td>-4.92800</td>\n",
              "      <td>12.88590</td>\n",
              "      <td>-11.87773</td>\n",
              "      <td>2.48031</td>\n",
              "      <td>-16.11028</td>\n",
              "      <td>-16.40421</td>\n",
              "      <td>-8.29657</td>\n",
              "      <td>9.86817</td>\n",
              "      <td>...</td>\n",
              "      <td>11.92816</td>\n",
              "      <td>-73.72412</td>\n",
              "      <td>16.19039</td>\n",
              "      <td>9.79606</td>\n",
              "      <td>9.71693</td>\n",
              "      <td>-9.90907</td>\n",
              "      <td>-20.65851</td>\n",
              "      <td>2.34002</td>\n",
              "      <td>-31.57015</td>\n",
              "      <td>1.58400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42c6aa0d-d1ca-4462-be29-bcabf093442d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42c6aa0d-d1ca-4462-be29-bcabf093442d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42c6aa0d-d1ca-4462-be29-bcabf093442d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "Explain why it would be problematic to have\n",
        "some songs from an artist in the training set, and other songs from the same artist in the\n",
        "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
        "will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NiYlxpFzGrB"
      },
      "source": [
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()\n",
        "\n",
        "# Explanation:\n",
        "# Since most of the songs of an artist have similar music style, it could be argued that splitting a single artist's songs into both \n",
        "# training and testing might lead to an over-confident result as the model might recognize the artist and immidietly classify the song into the dominant century\n",
        "# of the artist. The result is that song's century being classified according on the artist."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
        "has the *same* mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "source": [
        "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
        "feature_stds  = df_train.std()[1:].to_numpy()\n",
        "\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zmZk6ezGrC"
      },
      "source": [
        "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
        "This is *not* a bug.\n",
        "\n",
        "Explain why it would be improper to compute and use test set means\n",
        "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZy6brwzGrC"
      },
      "source": [
        "# Explanation:\n",
        "# The splitting between training and testing is \"artificial\" - the testing is supposed to represent the model's performence on new unknown data.\n",
        "# Thus, the only \"known\" parameters are the training parameters - the testing parameters are pretented to be hidden. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "\n",
        "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
        "set during the model building process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXv1U3gzGrC"
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
        "\n",
        "# Explanation:\n",
        "# We should limit how many times we use the test data, as we do not want to adjust the model specificly according to the test data (overfitting) because it would not allow us to examine\n",
        "# the accuracy and performance of our model later.\n",
        "# In order to validate a specific model structure (and specifically - the hyperparameters) we take a small portion of the training set - namely the \"validation\" set.\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification.\n",
        "These helper functions are written for you. All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6BA_s-kzGrD"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "def cross_entropy(t, y):\n",
        "  threshold = 10**-8\n",
        "  return -t * np.log(y + threshold) - (1 - t) * np.log(1 - y + threshold)\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def get_accuracy(y, t):\n",
        "  acc = 0\n",
        "  N = 0\n",
        "  for i in range(len(y)):\n",
        "    N += 1\n",
        "    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
        "      acc += 1\n",
        "  return acc / N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Write a function `pred` that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by: \n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naY5mT4_zGrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda9793e-a4f1-4a2c-b9ac-c017e219a988"
      },
      "source": [
        "def pred(w, b, X):\n",
        "  \"\"\"\n",
        "  Returns the prediction `y` of the target based on the weights `w` and scalar bias `b`.\n",
        "\n",
        "  Preconditions: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "                 np.shape(X) = (N, 90) for some N\n",
        "\n",
        "  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
        "  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last decimals\n",
        "  \"\"\"\n",
        "  # Your code goes here \n",
        "  y = sigmoid(np.dot(w, X.T) + b)\n",
        "  return y\n",
        "\n",
        "pred(np.zeros(90), 1, np.ones([2, 90]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.73105858])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "Write a function `derivative_cost` that computes and returns the gradients \n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80bu7qmzGrE"
      },
      "source": [
        "def derivative_cost(X, y, t):\n",
        "  \"\"\"\n",
        "  Returns a tuple containing the gradients dLdw and dLdb.\n",
        "\n",
        "  Precondition: np.shape(X) == (N, 90) for some N\n",
        "                np.shape(y) == (N,)\n",
        "                np.shape(t) == (N,)\n",
        "\n",
        "  Postcondition: np.shape(dLdw) = (90,)\n",
        "           type(dLdb) = float\n",
        "  \"\"\"\n",
        "  # Your code goes here\n",
        "  dLdw = np.dot((y-t),X) / len(X)\n",
        "  dLdb = np.mean(y-t)\n",
        "  return (dLdw, dLdb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPRGM3BjKe2"
      },
      "source": [
        "# **Explenation on Gradients**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHfmPVdsg0eX"
      },
      "source": [
        "**``An explaination on how the gradients are computed``**:\n",
        "\n",
        "Consider a batch of $N$ training datapoints which is composed of samples and their corresponding lables, i.e. $\\boldsymbol{\\mathcal D_i} = \\{(\\boldsymbol{x}_n, t_n)\\}^{N}_{n=1} \\subset \\boldsymbol{\\mathcal D_t}$.\n",
        "\n",
        "The predicted label of $\\boldsymbol{x}_n \\in \\mathbb{R}^{m}$ is given by -  \n",
        "$$\\hat{y}_n = \\sigma({\\boldsymbol w}^T {\\boldsymbol x}_n + b)$$\n",
        "\n",
        "Where $\\sigma : \\mathbb{R}^{m} \\rightarrow (0,1)$ is the sigmoid function.\n",
        "\n",
        "The corss entropy loss of this batch is given by the following expression-\n",
        "\n",
        "$$\\mathcal L_{\\theta}(\\boldsymbol t, \\boldsymbol{\\hat{y}}) = CE(\\boldsymbol t, \\boldsymbol{\\hat{y}}) = \\frac{1}{N}\\sum_{n=1}^{N} \\left[ -t_n \\cdot log(\\hat{y}_n)  - (1 - t_n) \\cdot log(1 - \\hat{y}_n) \\right]$$\n",
        "\n",
        "Where $\\theta = \\{\\boldsymbol w, b\\}$ are the objective parameters, i.e. the weights. \n",
        "\n",
        "The following derivation aims to extract nice and compact derivative expressions of $\\mathcal L_{\\theta}$ with respect to its objective parameters.\n",
        "\n",
        "**1. Derivation of $\\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial b}$ :**\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial b} = \n",
        "\\frac{1}{N}\\sum_{n=1}^{N} \\left[ -t_n \\cdot \\frac{\\partial log(\\hat{y}_n)}{\\partial b}  - (1 - t_n) \\cdot \\frac{\\partial log(1 - \\hat{y}_n)}{\\partial b} \\right]$$\n",
        "\n",
        "We begin with the chain rule, to break down the derivative dependecies -\n",
        "\n",
        "$$ \\frac{\\partial log(\\hat{y}_n)}{\\partial b} = \\frac{\\partial log(\\hat{y}_n)}{\\partial \\hat{y}_n} \\cdot\n",
        "\\frac{\\partial \\hat{y}_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial b}$$\n",
        "\n",
        "Where $z_n = {\\boldsymbol w}^T {\\boldsymbol x}_n + b$. The next step is writing the derivative terms explicitly -\n",
        "\n",
        "$$ \\frac{\\partial log(\\hat{y}_n)}{\\partial \\hat{y}_n} = \\frac{1}{\\hat{y}_n}$$\n",
        "\n",
        "$$ \\frac{\\partial z_n}{\\partial b} = 1$$\n",
        "\n",
        "In order to get an explicit term of $\\frac{\\partial \\hat{y}_n}{\\partial z_n}$, we will use an auxiliary equation derivation:\n",
        "\n",
        "-----------------------------------------------\n",
        "**Auxiliary Statement: Derivative of sigmoid function**\n",
        "\n",
        "$$\\frac{d \\sigma(z)}{dz} = \\frac{d}{dz}\\frac{1}{1 + e^{-z}} = \\frac{-\\frac{d}{dz}(1 + e^{-z})}{(1 + e^{-z})^2} = \\frac{e^{-z}}{(1 + e^{-z})^2} = \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}} = \\frac{1}{1 + e^{-z}} \\cdot \\frac{(1 + e^{-z}) - 1}{1 + e^{-z}} = \\frac{1}{1 + e^{-z}} \\cdot \\left(\\frac{1 + e^{-z}}{1 + e^{-z}} - \\frac{1}{1 + e^{-z}}\\right) = \\frac{1}{1 + e^{-z}} \\cdot \\left(1 - \\frac{1}{1 + e^{-z}}\\right) = \\sigma(z) \\cdot \\left(1 - \\sigma(z) \\right)$$\n",
        "\n",
        "----------------------------------\n",
        "\n",
        "Therefore it holds that-\n",
        "\n",
        "$$ \\frac{\\partial \\hat{y}_n}{\\partial z_n} = \\frac{d\\sigma(z_n)}{d z_n} = \\sigma(z_n) \\cdot \\left(1 - \\sigma(z_n) \\right) = \\hat{y}_n \\cdot \\left(1 - \\hat{y}_n \\right)$$\n",
        "\n",
        "Composing all together, we get-\n",
        "\n",
        "$$ \\frac{\\partial log(\\hat{y}_n)}{\\partial b} = \\frac{\\partial log(\\hat{y}_n)}{\\partial \\hat{y}_n} \\cdot\n",
        "\\frac{\\partial \\hat{y}_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial b} = \\frac{1}{\\hat{y}_n} \\cdot \\hat{y}_n \\cdot \\left(1 - \\hat{y}_n \\right) \\cdot 1 = 1 - \\hat{y}_n$$\n",
        "\n",
        "$$ \\frac{\\partial log(1 - \\hat{y}_n)}{\\partial b} = \\frac{\\partial log(1 - \\hat{y}_n)}{\\partial \\hat{y}_n} \\cdot\n",
        "\\frac{\\partial \\hat{y}_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial b} = \\frac{-1}{1 - \\hat{y}_n} \\cdot \\hat{y}_n \\cdot \\left(1 - \\hat{y}_n \\right) \\cdot 1 = - \\hat{y}_n$$\n",
        "\n",
        "Using these expressions, we get that the partial derivative $\\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial b}$ is-\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial b} = \n",
        "\\frac{1}{N}\\sum_{n=1}^{N} \\left[ -t_n \\cdot \\left(1 - \\hat{y}_n \\right) + (1 - t_n) \\cdot \\hat{y}_n  \\right]\n",
        "= \\frac{1}{N}\\sum_{n=1}^{N} \\left[\\hat{y}_n -t_n\\right]$$\n",
        "\n",
        "\n",
        "**2. Derivation of $\\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial {\\boldsymbol w}}$ :**\n",
        "\n",
        "Now, we have to get a nice expression of -\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial w_i} = \n",
        "\\frac{1}{N}\\sum_{n=1}^{N} \\left[ -t_n \\cdot \\frac{\\partial log(\\hat{y}_n)}{\\partial w_i}  - (1 - t_n) \\cdot \\frac{\\partial log(1 - \\hat{y}_n)}{\\partial w_i} \\right]$$\n",
        "\n",
        "Again, we break down the general derivative term with the chain rule -\n",
        "\n",
        "$$ \\frac{\\partial log(\\hat{y}_n)}{\\partial w_i} = \\frac{\\partial log(\\hat{y}_n)}{\\partial \\hat{y}_n} \\cdot\n",
        "\\frac{\\partial \\hat{y}_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial w_i}$$\n",
        "\n",
        "Only the last term has been changed comparing to the derivative by $b$, lets find it now-\n",
        "\n",
        "$$\\frac{\\partial z_n}{\\partial w_i} = \\frac{\\partial\\left({\\boldsymbol w}^T {\\boldsymbol x}_n + b\\right)}{\\partial w_i} = x_n^{(i)}$$\n",
        "\n",
        "Putting all together, we get-\n",
        "\n",
        "$$ \\frac{\\partial log(\\hat{y}_n)}{\\partial \\boldsymbol w} = \\frac{\\partial log(\\hat{y}_n)}{\\partial \\hat{y}_n} \\cdot\n",
        "\\frac{\\partial \\hat{y}_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial \\boldsymbol w} = \\frac{1}{\\hat{y}_n} \\cdot \\hat{y}_n \\cdot \\left(1 - \\hat{y}_n \\right) \\cdot \\boldsymbol{x}_n = \\left(1 - \\hat{y}_n \\right) \\cdot \\boldsymbol{x}_n$$\n",
        "\n",
        "$$ \\frac{\\partial log(1 - \\hat{y}_n)}{\\partial \\boldsymbol w} = \\frac{\\partial log(1 - \\hat{y}_n)}{\\partial \\hat{y}_n} \\cdot\n",
        "\\frac{\\partial \\hat{y}_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial \\boldsymbol w} = \\frac{-1}{1 - \\hat{y}_n} \\cdot \\hat{y}_n \\cdot \\left(1 - \\hat{y}_n \\right) \\cdot 1 = - \\hat{y}_n \\cdot \\boldsymbol{x}_n$$\n",
        "\n",
        "Using these expressions, we get that the partial derivative $\\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial \\boldsymbol w}$ is-\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}_{\\theta}}{\\partial \\boldsymbol w} = \n",
        "\\frac{1}{N}\\sum_{n=1}^{N} \\left[ -t_n \\cdot \\left(1 - \\hat{y}_n \\right) \\cdot \\boldsymbol{x}_n + (1 - t_n) \\cdot \\hat{y}_n \\cdot \\boldsymbol{x}_n \\right]\n",
        "= \\frac{1}{N}\\sum_{n=1}^{N} \\left[\\left(\\hat{y}_n -t_n\\right)\\cdot \\boldsymbol{x}_n\\right]\n",
        "=\\frac{1}{N} \\cdot\n",
        "\\left[\\begin{array}{cc} \n",
        "| &   & |\\\\\n",
        "\\boldsymbol{x}_1 & \\cdot\\cdot\\cdot &\\boldsymbol{x}_N\\\\\n",
        "| &   & |\\\\\n",
        "\\end{array}\\right] \\cdot\n",
        "\\left[\\begin{array}{cc} \n",
        "\\hat{y}_1 -t_1\\\\ \n",
        "\\vdots \\\\\n",
        "\\hat{y}_N -t_N \\\\\n",
        "\\end{array}\\right]\n",
        "$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRTD-fozGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f322f1a-3080-4c1d-8553-8d1747f3454f"
      },
      "source": [
        "# Your code goes here\n",
        "N = 90\n",
        "M = 1\n",
        "w = np.random.randn(N)\n",
        "b = np.random.randn(M)\n",
        "t = np.round(np.random.random(M))\n",
        "X = np.random.randn(M, N)\n",
        "h = 0.00000001\n",
        "\n",
        "y_b_plus_h = pred(w, b + h, X) \n",
        "y_b_ = pred(w, b, X)\n",
        "y_b = pred(w, b, X)\n",
        "\n",
        "r1 = derivative_cost(X, y_b, t)\n",
        "r2 = ((cross_entropy(t, y_b_plus_h) - cross_entropy(t, y_b_)) / h)[0]\n",
        "error = np.sum((r1[1] - r2) ** 2)\n",
        "\n",
        "print(\"The analytical results is -\", r1[1])\n",
        "print(\"The algorithm results is - \", r2)\n",
        "print(\"the MSE in dLdb: \", error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is - -0.0001041207825315249\n",
            "The algorithm results is -  -0.0001041053501000741\n",
            "the MSE in dLdb:  2.3815994048368486e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "### Part (d) -- 7%\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTsHgnPzGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b6042c-e06e-42f1-bb21-672648f18c2e"
      },
      "source": [
        "# Your code goes here. You might find this below code helpful: but it's\n",
        "# up to you to figure out how/why, and how to modify the code\n",
        "\n",
        "N = 90\n",
        "M = 1\n",
        "w = np.random.randn(N)\n",
        "b = np.random.randn(M)\n",
        "t = np.round(np.random.random(M))\n",
        "X = np.random.randn(M, N)\n",
        "h = 0.000001\n",
        "\n",
        "# Analytical derivative \n",
        "y_w = pred(w, b, X)\n",
        "r1 = derivative_cost(X, y_w, t)\n",
        "\n",
        "# Numerical derivative \n",
        "eps = np.eye(N) * h\n",
        "w_tile = np.tile(w, (N, 1))\n",
        "w_plus_h = w_tile + eps\n",
        "y_w_ = pred(w, b, X)\n",
        "y_w_plus_h = pred(w_plus_h[0], b, X)\n",
        "r2 = np.zeros(N)\n",
        "\n",
        "for i in range(N):\n",
        "    y_w_plus_h = pred(w_plus_h[i], b, X)\n",
        "    r2[i] = (cross_entropy(t, y_w_plus_h) - cross_entropy(t, y_w_)) / h\n",
        "\n",
        "print(\"The analytical results is -\", r1[0])\n",
        "print(\"The algorithm results is - \", r2)\n",
        "\n",
        "# error = np.transpose(r2) - np.transpose(r1[0])\n",
        "error = np.sum((r1[0] - r2) ** 2)\n",
        "print(\"the MSE in the derivative :\", error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is - [ 0.82726972 -0.66230709  0.25820897 -1.92980575  1.09529574  0.19762516\n",
            "  0.29857044 -1.38824009  1.01463334  1.44972645 -0.75347518  0.22327019\n",
            " -1.14655618 -1.6912748   0.74728249  0.71521807  1.96038417 -0.23089127\n",
            " -0.44537754 -1.03115872  1.07904223  0.17337235  0.29581535  0.09057069\n",
            "  1.18948366  1.08508526 -0.03067308 -2.62277022  1.15771782 -1.79495528\n",
            "  0.48264408  0.28309505 -0.10468104 -0.47972916  0.75352883 -0.74153987\n",
            "  0.50460298  1.33686457 -1.23589018 -0.40108464  1.08551031  0.34270424\n",
            " -1.7371821   0.35242195  2.21184114  1.19299147 -0.62329051  0.0613587\n",
            "  0.03790642  0.76473113 -1.12605376 -1.65709006  1.24068025  0.11510897\n",
            "  1.86137747 -0.11740348  1.2005525   0.43542239  2.07693958 -0.57452805\n",
            "  1.16523358  2.04951735  0.15638526 -0.76694317 -1.81831666 -1.34067065\n",
            "  0.11692267 -0.36365263 -1.10595748  1.02506437 -0.80874401  0.33794113\n",
            "  0.7843659   1.18650104 -0.50864935 -0.5120924  -0.22697816  0.59285405\n",
            "  0.40576617  1.06261681  1.03738553 -1.03854287 -0.44062003  0.94573609\n",
            " -0.0792915  -0.34435117  0.63265506 -2.28372358  0.16281058 -2.22724541]\n",
            "The algorithm results is -  [ 0.82726925 -0.66230715  0.25820823 -1.92980618  1.09529572  0.19762486\n",
            "  0.29857079 -1.38824024  1.01463323  1.44972597 -0.75347538  0.22326965\n",
            " -1.14655669 -1.69127524  0.74728247  0.71521826  1.96038395 -0.23089143\n",
            " -0.44537753 -1.03115947  1.0790423   0.17337224  0.29581547  0.09057045\n",
            "  1.18948323  1.08508516 -0.03067286 -2.62277011  1.15771771 -1.79495567\n",
            "  0.48264377  0.28309447 -0.10468109 -0.47972918  0.75352854 -0.74153988\n",
            "  0.50460303  1.33686417 -1.23589052 -0.40108503  1.08550968  0.34270387\n",
            " -1.73718242  0.35242175  2.21184133  1.1929908  -0.62329053  0.06135876\n",
            "  0.03790594  0.76473074 -1.12605359 -1.65709024  1.24067989  0.11510916\n",
            "  1.86137781 -0.11740377  1.20055248  0.43542214  2.07693979 -0.57452869\n",
            "  1.16523353  2.04951696  0.15638468 -0.76694315 -1.81831715 -1.34067077\n",
            "  0.11692248 -0.36365284 -1.10595817  1.02506424 -0.80874454  0.33794044\n",
            "  0.7843655   1.18650071 -0.50864929 -0.51209249 -0.22697828  0.59285365\n",
            "  0.40576574  1.06261681  1.03738515 -1.03854316 -0.44061999  0.94573588\n",
            " -0.07929126 -0.34435089  0.63265497 -2.28372406  0.16280997 -2.22724582]\n",
            "the MSE in the derivative : 1.0757540800641915e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### Part (e) -- 7%\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent. \n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "source": [
        "def run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu=0.1, batch_size=100, max_iters=100):\n",
        "  w = w0\n",
        "  b = b0\n",
        "  iter = 0\n",
        "  train_cost_arr = []\n",
        "  train_acc_arr = []\n",
        "  \n",
        "  val_cost_arr = []\n",
        "  val_acc_arr = []\n",
        "\n",
        "  X_val = val_norm_xs\n",
        "  t_val = val_ts.squeeze()\n",
        "\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    reindex = np.random.permutation(len(train_norm_xs))\n",
        "    train_norm_xs = train_norm_xs[reindex]\n",
        "    train_ts = train_ts[reindex]\n",
        "\n",
        "    # iterate over each minibatch\n",
        "    for i in range(0, len(train_norm_xs), batch_size):\n",
        "      # minibatch that we are working with for training:\n",
        "      X = train_norm_xs[i:(i + batch_size)]\n",
        "      t = train_ts[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      y = pred(w, b, X)\n",
        "\n",
        "      # compute costs and append to lists\n",
        "      train_cost = cost(y, t)\n",
        "      train_cost_arr.append(train_cost)\n",
        "\n",
        "      # compute accuracy and append to lists\n",
        "      train_acc = get_accuracy(y, t)\n",
        "      train_acc_arr.append(train_acc)\n",
        "\n",
        "      # validation calculations\n",
        "      y_val = pred(w, b, X_val)\n",
        "\n",
        "      val_cost = cost(y_val, t_val)\n",
        "      val_cost_arr.append(val_cost)\n",
        "      val_acc = get_accuracy(y_val, t_val)\n",
        "      val_acc_arr.append(val_acc)\n",
        "\n",
        "      # update w and b\n",
        "      dLdw, dLdb = derivative_cost(X, y, t)\n",
        "      w = w - mu * dLdw\n",
        "      b = b - mu * dLdb\n",
        "        \n",
        "      # increment the iteration count\n",
        "      iter += 1\n",
        "\n",
        "      # print the *validation* loss and accuracy\n",
        "      if iter % 10 == 0:\n",
        "        print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (iter, val_acc * 100, val_cost))\n",
        "    \n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "  \n",
        "  return w, b, train_cost_arr, train_acc_arr, val_cost_arr, val_acc_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### Part (f) -- 7%\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
        "Show that if the learning rate $\\mu$ is too small, then convergence is slow.\n",
        "Also, show that if $\\mu$ is too large, then the optimization algorirthm does not converge. The demonstration should be made using plots showing these effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE32Iqo6zGrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8a4b78-b59e-4641-b360-dbb4cd575982"
      },
      "source": [
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "mu_arr = [0.01, 0.1, 1, 5]\n",
        "batch_size = 100 \n",
        "max_iters = 100\n",
        "val_acc = []\n",
        "val_cost = []\n",
        "\n",
        "index = -1\n",
        "for mu in mu_arr:\n",
        "  index += 1\n",
        "  w, b, costs, accs, val_cost_tmp, val_acc_tmp = run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu, batch_size, max_iters)\n",
        "  val_acc.append(val_acc_tmp)\n",
        "  val_cost.append(val_cost_tmp)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 10. [Val Acc 52%, Loss 2.856784]\n",
            "Iter 20. [Val Acc 52%, Loss 2.832289]\n",
            "Iter 30. [Val Acc 52%, Loss 2.809152]\n",
            "Iter 40. [Val Acc 52%, Loss 2.787986]\n",
            "Iter 50. [Val Acc 52%, Loss 2.763330]\n",
            "Iter 60. [Val Acc 52%, Loss 2.739893]\n",
            "Iter 70. [Val Acc 52%, Loss 2.717298]\n",
            "Iter 80. [Val Acc 52%, Loss 2.694195]\n",
            "Iter 90. [Val Acc 52%, Loss 2.674377]\n",
            "Iter 100. [Val Acc 52%, Loss 2.653272]\n",
            "Iter 10. [Val Acc 52%, Loss 2.664209]\n",
            "Iter 20. [Val Acc 53%, Loss 2.498457]\n",
            "Iter 30. [Val Acc 53%, Loss 2.332489]\n",
            "Iter 40. [Val Acc 54%, Loss 2.151900]\n",
            "Iter 50. [Val Acc 55%, Loss 1.999708]\n",
            "Iter 60. [Val Acc 56%, Loss 1.882810]\n",
            "Iter 70. [Val Acc 57%, Loss 1.767562]\n",
            "Iter 80. [Val Acc 57%, Loss 1.667827]\n",
            "Iter 90. [Val Acc 58%, Loss 1.561525]\n",
            "Iter 100. [Val Acc 59%, Loss 1.479818]\n",
            "Iter 10. [Val Acc 56%, Loss 1.775514]\n",
            "Iter 20. [Val Acc 61%, Loss 1.176912]\n",
            "Iter 30. [Val Acc 66%, Loss 0.929776]\n",
            "Iter 40. [Val Acc 66%, Loss 0.796828]\n",
            "Iter 50. [Val Acc 68%, Loss 0.710858]\n",
            "Iter 60. [Val Acc 65%, Loss 0.870308]\n",
            "Iter 70. [Val Acc 70%, Loss 0.677020]\n",
            "Iter 80. [Val Acc 70%, Loss 0.657346]\n",
            "Iter 90. [Val Acc 70%, Loss 0.643529]\n",
            "Iter 100. [Val Acc 69%, Loss 0.706589]\n",
            "Iter 10. [Val Acc 55%, Loss 3.052212]\n",
            "Iter 20. [Val Acc 59%, Loss 2.766588]\n",
            "Iter 30. [Val Acc 65%, Loss 2.011878]\n",
            "Iter 40. [Val Acc 64%, Loss 1.858036]\n",
            "Iter 50. [Val Acc 63%, Loss 2.345577]\n",
            "Iter 60. [Val Acc 57%, Loss 3.025352]\n",
            "Iter 70. [Val Acc 67%, Loss 1.542770]\n",
            "Iter 80. [Val Acc 64%, Loss 1.685689]\n",
            "Iter 90. [Val Acc 62%, Loss 2.598426]\n",
            "Iter 100. [Val Acc 67%, Loss 1.571149]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (cost_fig, accuracy_fig) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "\n",
        "iter_axis = range(0, len(val_acc[0]))\n",
        "\n",
        "cost_fig.plot(iter_axis, val_acc[0], iter_axis, val_acc[1], iter_axis, val_acc[2], iter_axis, val_acc[3])\n",
        "cost_fig.set_xlabel('Iterations')\n",
        "cost_fig.set_ylabel('Accuracy')\n",
        "cost_fig.legend(['\\u03BC =' + str(mu_arr[0]), '\\u03BC =' + str(mu_arr[1]), '\\u03BC =' + str(mu_arr[2]), '\\u03BC =' + str(mu_arr[3])])\n",
        "\n",
        "accuracy_fig.plot(iter_axis, val_cost[0], iter_axis, val_cost[1], iter_axis, val_cost[2], iter_axis, val_cost[3])\n",
        "accuracy_fig.set_xlabel('Iterations')\n",
        "accuracy_fig.set_ylabel('Cost')\n",
        "accuracy_fig.legend(['\\u03BC =' + str(mu_arr[0]), '\\u03BC =' + str(mu_arr[1]), '\\u03BC =' + str(mu_arr[2]), '\\u03BC =' + str(mu_arr[3])])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QnG2kk934k2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8c981285-02f2-491e-97cd-31bb11f32ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S44GmUZ12o-g"
      },
      "source": [
        "**Explain and discuss your results here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that when the step size mu is too small (0.01) the accuracy is not improving during the iterations.\n",
        "as a result the cost function stays relativly high.\n",
        "when the step size mu is too large (5) the cost function does not converge, it has big oscillations- the model dosn't find the local minimum of the cost function.\n",
        "In between, we can see that for mu = 0.1 the results are better then the 0.01 but still not fast enough to converge.\n",
        "For mu=1 the accuracy is graduly increaing while the loss function sets on a local minimum.  \n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "A4ig9XB36ZOB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### Part (g) -- 7%\n",
        "\n",
        "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
        "the learning rate $\\mu$ and the batch size. Show plots demostrating good and bad behaviours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFOFSwgzGrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b33d0f-5fcb-4d8c-f478-52cee1f6fe45"
      },
      "source": [
        "# Hyperparameters Grid Search (Criterion: Maximum Accuracy)\n",
        "\n",
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "mu_start = 0.2\n",
        "mu_end = 5\n",
        "mu_res = 0.4\n",
        "mu_vec = np.arange(mu_start, mu_end, mu_res)\n",
        "\n",
        "batch_start = 50\n",
        "batch_end = 500\n",
        "batch_res = 50\n",
        "batch_vec = np.arange(batch_start, batch_end, batch_res)\n",
        "acc_mat = np.zeros([mu_vec.shape[0],batch_vec.shape[0]])\n",
        "opt_acc = 0\n",
        "b_opt = b0\n",
        "w_opt = w0\n",
        "curr_val_acc = 0 \n",
        "\n",
        "for mu in enumerate(mu_vec):\n",
        "    for batch in enumerate(batch_vec):\n",
        "      w, b, train_cost_arr, train_acc_arr, val_cost_arr_iter, val_acc_arr_iter = run_gradient_descent(train_norm_xs[:100000],train_ts[:100000],\n",
        "                                                val_norm_xs[:3000],val_ts[:3000],w0, b0, mu=mu[1], batch_size=batch[1], max_iters=100)\n",
        "      curr_val_acc = val_acc_arr_iter[-1]\n",
        "        \n",
        "      if curr_val_acc > opt_acc:\n",
        "        opt_acc = curr_val_acc\n",
        "        opt_cost = val_cost_arr_iter[-1]\n",
        "        b_opt = b\n",
        "        w_opt = w\n",
        "        mu_opt = mu[1]\n",
        "        batch_opt = batch[1]\n",
        "            \n",
        "      acc_mat[mu[0],batch[0]] = curr_val_acc\n",
        "\n",
        "print('optimal cost = ',opt_cost, ', optimal accuracy = ', opt_acc, ', optimal mu = ', mu_opt, ', optimal batch = ', batch_opt)\n",
        "print('optimal b = ', b_opt, ', optimal w = ', w_opt)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 10. [Val Acc 49%, Loss 3.069969]\n",
            "Iter 20. [Val Acc 51%, Loss 2.691297]\n",
            "Iter 30. [Val Acc 52%, Loss 2.449990]\n",
            "Iter 40. [Val Acc 52%, Loss 2.187037]\n",
            "Iter 50. [Val Acc 55%, Loss 1.906310]\n",
            "Iter 60. [Val Acc 56%, Loss 1.731245]\n",
            "Iter 70. [Val Acc 57%, Loss 1.539944]\n",
            "Iter 80. [Val Acc 58%, Loss 1.406498]\n",
            "Iter 90. [Val Acc 58%, Loss 1.325422]\n",
            "Iter 100. [Val Acc 60%, Loss 1.194919]\n",
            "Iter 10. [Val Acc 49%, Loss 3.075241]\n",
            "Iter 20. [Val Acc 50%, Loss 2.660870]\n",
            "Iter 30. [Val Acc 51%, Loss 2.352241]\n",
            "Iter 40. [Val Acc 52%, Loss 2.113538]\n",
            "Iter 50. [Val Acc 54%, Loss 1.906134]\n",
            "Iter 60. [Val Acc 56%, Loss 1.719008]\n",
            "Iter 70. [Val Acc 57%, Loss 1.544259]\n",
            "Iter 80. [Val Acc 58%, Loss 1.404947]\n",
            "Iter 90. [Val Acc 59%, Loss 1.273066]\n",
            "Iter 100. [Val Acc 60%, Loss 1.170344]\n",
            "Iter 10. [Val Acc 49%, Loss 3.062444]\n",
            "Iter 20. [Val Acc 51%, Loss 2.666759]\n",
            "Iter 30. [Val Acc 52%, Loss 2.334590]\n",
            "Iter 40. [Val Acc 53%, Loss 2.079000]\n",
            "Iter 50. [Val Acc 55%, Loss 1.839531]\n",
            "Iter 60. [Val Acc 56%, Loss 1.649323]\n",
            "Iter 70. [Val Acc 58%, Loss 1.502411]\n",
            "Iter 80. [Val Acc 59%, Loss 1.359869]\n",
            "Iter 90. [Val Acc 58%, Loss 1.239170]\n",
            "Iter 100. [Val Acc 60%, Loss 1.152540]\n",
            "Iter 10. [Val Acc 49%, Loss 3.069414]\n",
            "Iter 20. [Val Acc 51%, Loss 2.660235]\n",
            "Iter 30. [Val Acc 52%, Loss 2.332369]\n",
            "Iter 40. [Val Acc 54%, Loss 2.056798]\n",
            "Iter 50. [Val Acc 55%, Loss 1.843934]\n",
            "Iter 60. [Val Acc 57%, Loss 1.658462]\n",
            "Iter 70. [Val Acc 58%, Loss 1.499076]\n",
            "Iter 80. [Val Acc 59%, Loss 1.380496]\n",
            "Iter 90. [Val Acc 60%, Loss 1.263343]\n",
            "Iter 100. [Val Acc 61%, Loss 1.167261]\n",
            "Iter 10. [Val Acc 49%, Loss 3.050810]\n",
            "Iter 20. [Val Acc 51%, Loss 2.625185]\n",
            "Iter 30. [Val Acc 52%, Loss 2.331117]\n",
            "Iter 40. [Val Acc 53%, Loss 2.077719]\n",
            "Iter 50. [Val Acc 55%, Loss 1.842690]\n",
            "Iter 60. [Val Acc 56%, Loss 1.657957]\n",
            "Iter 70. [Val Acc 58%, Loss 1.491158]\n",
            "Iter 80. [Val Acc 59%, Loss 1.355796]\n",
            "Iter 90. [Val Acc 59%, Loss 1.237584]\n",
            "Iter 100. [Val Acc 60%, Loss 1.154729]\n",
            "Iter 10. [Val Acc 49%, Loss 3.060233]\n",
            "Iter 20. [Val Acc 51%, Loss 2.624827]\n",
            "Iter 30. [Val Acc 51%, Loss 2.322823]\n",
            "Iter 40. [Val Acc 53%, Loss 2.048754]\n",
            "Iter 50. [Val Acc 55%, Loss 1.815381]\n",
            "Iter 60. [Val Acc 57%, Loss 1.633701]\n",
            "Iter 70. [Val Acc 58%, Loss 1.471300]\n",
            "Iter 80. [Val Acc 59%, Loss 1.344279]\n",
            "Iter 90. [Val Acc 59%, Loss 1.243693]\n",
            "Iter 100. [Val Acc 60%, Loss 1.162709]\n",
            "Iter 10. [Val Acc 49%, Loss 3.045685]\n",
            "Iter 20. [Val Acc 50%, Loss 2.646625]\n",
            "Iter 30. [Val Acc 52%, Loss 2.329441]\n",
            "Iter 40. [Val Acc 53%, Loss 2.068010]\n",
            "Iter 50. [Val Acc 55%, Loss 1.844339]\n",
            "Iter 60. [Val Acc 56%, Loss 1.651376]\n",
            "Iter 70. [Val Acc 58%, Loss 1.502906]\n",
            "Iter 80. [Val Acc 59%, Loss 1.365505]\n",
            "Iter 90. [Val Acc 59%, Loss 1.237186]\n",
            "Iter 100. [Val Acc 60%, Loss 1.144760]\n",
            "Iter 10. [Val Acc 49%, Loss 3.050504]\n",
            "Iter 20. [Val Acc 51%, Loss 2.656849]\n",
            "Iter 30. [Val Acc 52%, Loss 2.326614]\n",
            "Iter 40. [Val Acc 53%, Loss 2.070306]\n",
            "Iter 50. [Val Acc 55%, Loss 1.842846]\n",
            "Iter 60. [Val Acc 56%, Loss 1.652059]\n",
            "Iter 70. [Val Acc 58%, Loss 1.493248]\n",
            "Iter 80. [Val Acc 59%, Loss 1.344776]\n",
            "Iter 90. [Val Acc 59%, Loss 1.240745]\n",
            "Iter 100. [Val Acc 60%, Loss 1.150246]\n",
            "Iter 10. [Val Acc 49%, Loss 3.060328]\n",
            "Iter 20. [Val Acc 51%, Loss 2.647974]\n",
            "Iter 30. [Val Acc 52%, Loss 2.335948]\n",
            "Iter 40. [Val Acc 53%, Loss 2.080623]\n",
            "Iter 50. [Val Acc 55%, Loss 1.861698]\n",
            "Iter 60. [Val Acc 56%, Loss 1.668787]\n",
            "Iter 70. [Val Acc 58%, Loss 1.498733]\n",
            "Iter 80. [Val Acc 58%, Loss 1.365565]\n",
            "Iter 90. [Val Acc 59%, Loss 1.248776]\n",
            "Iter 100. [Val Acc 60%, Loss 1.157478]\n",
            "Iter 10. [Val Acc 52%, Loss 2.470997]\n",
            "Iter 20. [Val Acc 55%, Loss 1.824903]\n",
            "Iter 30. [Val Acc 59%, Loss 1.542414]\n",
            "Iter 40. [Val Acc 61%, Loss 1.214160]\n",
            "Iter 50. [Val Acc 63%, Loss 0.964530]\n",
            "Iter 60. [Val Acc 63%, Loss 0.932674]\n",
            "Iter 70. [Val Acc 65%, Loss 0.838926]\n",
            "Iter 80. [Val Acc 67%, Loss 0.786459]\n",
            "Iter 90. [Val Acc 67%, Loss 0.800276]\n",
            "Iter 100. [Val Acc 68%, Loss 0.741492]\n",
            "Iter 10. [Val Acc 51%, Loss 2.587143]\n",
            "Iter 20. [Val Acc 56%, Loss 1.821497]\n",
            "Iter 30. [Val Acc 59%, Loss 1.346676]\n",
            "Iter 40. [Val Acc 61%, Loss 1.115244]\n",
            "Iter 50. [Val Acc 62%, Loss 0.977422]\n",
            "Iter 60. [Val Acc 64%, Loss 0.870032]\n",
            "Iter 70. [Val Acc 65%, Loss 0.803303]\n",
            "Iter 80. [Val Acc 65%, Loss 0.787978]\n",
            "Iter 90. [Val Acc 67%, Loss 0.730433]\n",
            "Iter 100. [Val Acc 68%, Loss 0.702142]\n",
            "Iter 10. [Val Acc 53%, Loss 2.333378]\n",
            "Iter 20. [Val Acc 57%, Loss 1.679612]\n",
            "Iter 30. [Val Acc 60%, Loss 1.269994]\n",
            "Iter 40. [Val Acc 62%, Loss 1.050007]\n",
            "Iter 50. [Val Acc 64%, Loss 0.922215]\n",
            "Iter 60. [Val Acc 65%, Loss 0.875402]\n",
            "Iter 70. [Val Acc 67%, Loss 0.775867]\n",
            "Iter 80. [Val Acc 66%, Loss 0.738202]\n",
            "Iter 90. [Val Acc 67%, Loss 0.699792]\n",
            "Iter 100. [Val Acc 67%, Loss 0.700313]\n",
            "Iter 10. [Val Acc 51%, Loss 2.404189]\n",
            "Iter 20. [Val Acc 56%, Loss 1.710658]\n",
            "Iter 30. [Val Acc 60%, Loss 1.276594]\n",
            "Iter 40. [Val Acc 62%, Loss 1.061163]\n",
            "Iter 50. [Val Acc 64%, Loss 0.898230]\n",
            "Iter 60. [Val Acc 65%, Loss 0.812124]\n",
            "Iter 70. [Val Acc 66%, Loss 0.745308]\n",
            "Iter 80. [Val Acc 67%, Loss 0.704609]\n",
            "Iter 90. [Val Acc 69%, Loss 0.700432]\n",
            "Iter 100. [Val Acc 70%, Loss 0.646773]\n",
            "Iter 10. [Val Acc 52%, Loss 2.373362]\n",
            "Iter 20. [Val Acc 56%, Loss 1.724630]\n",
            "Iter 30. [Val Acc 60%, Loss 1.279080]\n",
            "Iter 40. [Val Acc 61%, Loss 1.058596]\n",
            "Iter 50. [Val Acc 64%, Loss 0.910225]\n",
            "Iter 60. [Val Acc 65%, Loss 0.814237]\n",
            "Iter 70. [Val Acc 66%, Loss 0.759556]\n",
            "Iter 80. [Val Acc 67%, Loss 0.710761]\n",
            "Iter 90. [Val Acc 69%, Loss 0.686708]\n",
            "Iter 100. [Val Acc 69%, Loss 0.662684]\n",
            "Iter 10. [Val Acc 50%, Loss 2.431999]\n",
            "Iter 20. [Val Acc 56%, Loss 1.679288]\n",
            "Iter 30. [Val Acc 59%, Loss 1.255513]\n",
            "Iter 40. [Val Acc 62%, Loss 1.039577]\n",
            "Iter 50. [Val Acc 64%, Loss 0.896572]\n",
            "Iter 60. [Val Acc 66%, Loss 0.814682]\n",
            "Iter 70. [Val Acc 67%, Loss 0.756683]\n",
            "Iter 80. [Val Acc 67%, Loss 0.707275]\n",
            "Iter 90. [Val Acc 68%, Loss 0.678510]\n",
            "Iter 100. [Val Acc 69%, Loss 0.658719]\n",
            "Iter 10. [Val Acc 52%, Loss 2.352579]\n",
            "Iter 20. [Val Acc 56%, Loss 1.720160]\n",
            "Iter 30. [Val Acc 60%, Loss 1.271933]\n",
            "Iter 40. [Val Acc 62%, Loss 1.034561]\n",
            "Iter 50. [Val Acc 64%, Loss 0.890581]\n",
            "Iter 60. [Val Acc 65%, Loss 0.807711]\n",
            "Iter 70. [Val Acc 66%, Loss 0.755972]\n",
            "Iter 80. [Val Acc 66%, Loss 0.707310]\n",
            "Iter 90. [Val Acc 69%, Loss 0.677869]\n",
            "Iter 100. [Val Acc 69%, Loss 0.655423]\n",
            "Iter 10. [Val Acc 52%, Loss 2.397757]\n",
            "Iter 20. [Val Acc 57%, Loss 1.683480]\n",
            "Iter 30. [Val Acc 59%, Loss 1.304196]\n",
            "Iter 40. [Val Acc 61%, Loss 1.042727]\n",
            "Iter 50. [Val Acc 64%, Loss 0.893970]\n",
            "Iter 60. [Val Acc 65%, Loss 0.818490]\n",
            "Iter 70. [Val Acc 66%, Loss 0.749286]\n",
            "Iter 80. [Val Acc 67%, Loss 0.706961]\n",
            "Iter 90. [Val Acc 68%, Loss 0.672650]\n",
            "Iter 100. [Val Acc 69%, Loss 0.651455]\n",
            "Iter 10. [Val Acc 51%, Loss 2.427616]\n",
            "Iter 20. [Val Acc 56%, Loss 1.682175]\n",
            "Iter 30. [Val Acc 59%, Loss 1.252225]\n",
            "Iter 40. [Val Acc 61%, Loss 1.019456]\n",
            "Iter 50. [Val Acc 64%, Loss 0.878209]\n",
            "Iter 60. [Val Acc 65%, Loss 0.802480]\n",
            "Iter 70. [Val Acc 66%, Loss 0.745672]\n",
            "Iter 80. [Val Acc 67%, Loss 0.702898]\n",
            "Iter 90. [Val Acc 68%, Loss 0.680744]\n",
            "Iter 100. [Val Acc 68%, Loss 0.655614]\n",
            "Iter 10. [Val Acc 55%, Loss 2.148836]\n",
            "Iter 20. [Val Acc 61%, Loss 1.425580]\n",
            "Iter 30. [Val Acc 63%, Loss 1.089913]\n",
            "Iter 40. [Val Acc 64%, Loss 1.015793]\n",
            "Iter 50. [Val Acc 66%, Loss 0.988927]\n",
            "Iter 60. [Val Acc 66%, Loss 0.889860]\n",
            "Iter 70. [Val Acc 67%, Loss 0.849048]\n",
            "Iter 80. [Val Acc 67%, Loss 0.801281]\n",
            "Iter 90. [Val Acc 67%, Loss 0.768036]\n",
            "Iter 100. [Val Acc 62%, Loss 0.954517]\n",
            "Iter 10. [Val Acc 55%, Loss 1.957440]\n",
            "Iter 20. [Val Acc 61%, Loss 1.268681]\n",
            "Iter 30. [Val Acc 64%, Loss 0.988436]\n",
            "Iter 40. [Val Acc 66%, Loss 0.860263]\n",
            "Iter 50. [Val Acc 66%, Loss 0.789475]\n",
            "Iter 60. [Val Acc 66%, Loss 0.753809]\n",
            "Iter 70. [Val Acc 68%, Loss 0.724087]\n",
            "Iter 80. [Val Acc 68%, Loss 0.725859]\n",
            "Iter 90. [Val Acc 69%, Loss 0.702841]\n",
            "Iter 100. [Val Acc 64%, Loss 0.813042]\n",
            "Iter 10. [Val Acc 55%, Loss 1.933293]\n",
            "Iter 20. [Val Acc 60%, Loss 1.270236]\n",
            "Iter 30. [Val Acc 63%, Loss 0.960222]\n",
            "Iter 40. [Val Acc 65%, Loss 0.822470]\n",
            "Iter 50. [Val Acc 66%, Loss 0.760976]\n",
            "Iter 60. [Val Acc 67%, Loss 0.748035]\n",
            "Iter 70. [Val Acc 69%, Loss 0.712633]\n",
            "Iter 80. [Val Acc 69%, Loss 0.669026]\n",
            "Iter 90. [Val Acc 68%, Loss 0.704437]\n",
            "Iter 100. [Val Acc 68%, Loss 0.651649]\n",
            "Iter 10. [Val Acc 55%, Loss 1.967370]\n",
            "Iter 20. [Val Acc 61%, Loss 1.271723]\n",
            "Iter 30. [Val Acc 64%, Loss 0.936330]\n",
            "Iter 40. [Val Acc 65%, Loss 0.801729]\n",
            "Iter 50. [Val Acc 68%, Loss 0.722279]\n",
            "Iter 60. [Val Acc 68%, Loss 0.689130]\n",
            "Iter 70. [Val Acc 68%, Loss 0.678755]\n",
            "Iter 80. [Val Acc 69%, Loss 0.665584]\n",
            "Iter 90. [Val Acc 70%, Loss 0.638309]\n",
            "Iter 100. [Val Acc 69%, Loss 0.656265]\n",
            "Iter 10. [Val Acc 54%, Loss 2.005331]\n",
            "Iter 20. [Val Acc 60%, Loss 1.223546]\n",
            "Iter 30. [Val Acc 64%, Loss 0.936608]\n",
            "Iter 40. [Val Acc 64%, Loss 0.797720]\n",
            "Iter 50. [Val Acc 67%, Loss 0.703790]\n",
            "Iter 60. [Val Acc 67%, Loss 0.677633]\n",
            "Iter 70. [Val Acc 68%, Loss 0.651191]\n",
            "Iter 80. [Val Acc 70%, Loss 0.633719]\n",
            "Iter 90. [Val Acc 70%, Loss 0.615876]\n",
            "Iter 100. [Val Acc 70%, Loss 0.624117]\n",
            "Iter 10. [Val Acc 55%, Loss 1.893571]\n",
            "Iter 20. [Val Acc 61%, Loss 1.162451]\n",
            "Iter 30. [Val Acc 64%, Loss 0.896981]\n",
            "Iter 40. [Val Acc 66%, Loss 0.785996]\n",
            "Iter 50. [Val Acc 69%, Loss 0.704843]\n",
            "Iter 60. [Val Acc 69%, Loss 0.671921]\n",
            "Iter 70. [Val Acc 69%, Loss 0.655265]\n",
            "Iter 80. [Val Acc 70%, Loss 0.629920]\n",
            "Iter 90. [Val Acc 70%, Loss 0.613232]\n",
            "Iter 100. [Val Acc 70%, Loss 0.628796]\n",
            "Iter 10. [Val Acc 53%, Loss 1.995136]\n",
            "Iter 20. [Val Acc 61%, Loss 1.179976]\n",
            "Iter 30. [Val Acc 64%, Loss 0.913918]\n",
            "Iter 40. [Val Acc 66%, Loss 0.788862]\n",
            "Iter 50. [Val Acc 67%, Loss 0.744231]\n",
            "Iter 60. [Val Acc 68%, Loss 0.661041]\n",
            "Iter 70. [Val Acc 70%, Loss 0.641509]\n",
            "Iter 80. [Val Acc 70%, Loss 0.621013]\n",
            "Iter 90. [Val Acc 70%, Loss 0.631895]\n",
            "Iter 100. [Val Acc 70%, Loss 0.607433]\n",
            "Iter 10. [Val Acc 54%, Loss 1.936969]\n",
            "Iter 20. [Val Acc 60%, Loss 1.167288]\n",
            "Iter 30. [Val Acc 63%, Loss 0.893138]\n",
            "Iter 40. [Val Acc 66%, Loss 0.768176]\n",
            "Iter 50. [Val Acc 68%, Loss 0.696175]\n",
            "Iter 60. [Val Acc 69%, Loss 0.659109]\n",
            "Iter 70. [Val Acc 69%, Loss 0.649501]\n",
            "Iter 80. [Val Acc 70%, Loss 0.619861]\n",
            "Iter 90. [Val Acc 70%, Loss 0.615945]\n",
            "Iter 100. [Val Acc 70%, Loss 0.620689]\n",
            "Iter 10. [Val Acc 54%, Loss 1.948569]\n",
            "Iter 20. [Val Acc 61%, Loss 1.239043]\n",
            "Iter 30. [Val Acc 65%, Loss 0.894695]\n",
            "Iter 40. [Val Acc 67%, Loss 0.760834]\n",
            "Iter 50. [Val Acc 68%, Loss 0.701373]\n",
            "Iter 60. [Val Acc 69%, Loss 0.653025]\n",
            "Iter 70. [Val Acc 70%, Loss 0.622205]\n",
            "Iter 80. [Val Acc 70%, Loss 0.621886]\n",
            "Iter 90. [Val Acc 71%, Loss 0.619881]\n",
            "Iter 100. [Val Acc 71%, Loss 0.603663]\n",
            "Iter 10. [Val Acc 55%, Loss 1.931730]\n",
            "Iter 20. [Val Acc 61%, Loss 1.339864]\n",
            "Iter 30. [Val Acc 64%, Loss 1.055831]\n",
            "Iter 40. [Val Acc 67%, Loss 0.885618]\n",
            "Iter 50. [Val Acc 65%, Loss 0.926986]\n",
            "Iter 60. [Val Acc 64%, Loss 1.111548]\n",
            "Iter 70. [Val Acc 66%, Loss 0.894151]\n",
            "Iter 80. [Val Acc 65%, Loss 0.895574]\n",
            "Iter 90. [Val Acc 62%, Loss 1.278403]\n",
            "Iter 100. [Val Acc 61%, Loss 1.100481]\n",
            "Iter 10. [Val Acc 54%, Loss 1.816216]\n",
            "Iter 20. [Val Acc 62%, Loss 1.251429]\n",
            "Iter 30. [Val Acc 65%, Loss 0.960870]\n",
            "Iter 40. [Val Acc 65%, Loss 0.896497]\n",
            "Iter 50. [Val Acc 67%, Loss 0.811814]\n",
            "Iter 60. [Val Acc 63%, Loss 0.855351]\n",
            "Iter 70. [Val Acc 68%, Loss 0.721929]\n",
            "Iter 80. [Val Acc 67%, Loss 0.741771]\n",
            "Iter 90. [Val Acc 67%, Loss 0.724798]\n",
            "Iter 100. [Val Acc 66%, Loss 0.758821]\n",
            "Iter 10. [Val Acc 57%, Loss 1.750486]\n",
            "Iter 20. [Val Acc 62%, Loss 1.173790]\n",
            "Iter 30. [Val Acc 66%, Loss 0.859367]\n",
            "Iter 40. [Val Acc 68%, Loss 0.746177]\n",
            "Iter 50. [Val Acc 68%, Loss 0.711814]\n",
            "Iter 60. [Val Acc 70%, Loss 0.665269]\n",
            "Iter 70. [Val Acc 68%, Loss 0.680498]\n",
            "Iter 80. [Val Acc 65%, Loss 0.743882]\n",
            "Iter 90. [Val Acc 68%, Loss 0.709863]\n",
            "Iter 100. [Val Acc 61%, Loss 0.967203]\n",
            "Iter 10. [Val Acc 57%, Loss 1.647281]\n",
            "Iter 20. [Val Acc 62%, Loss 1.050770]\n",
            "Iter 30. [Val Acc 65%, Loss 0.841570]\n",
            "Iter 40. [Val Acc 66%, Loss 0.749537]\n",
            "Iter 50. [Val Acc 68%, Loss 0.712601]\n",
            "Iter 60. [Val Acc 66%, Loss 0.801583]\n",
            "Iter 70. [Val Acc 67%, Loss 0.696469]\n",
            "Iter 80. [Val Acc 70%, Loss 0.657365]\n",
            "Iter 90. [Val Acc 69%, Loss 0.657514]\n",
            "Iter 100. [Val Acc 70%, Loss 0.646126]\n",
            "Iter 10. [Val Acc 56%, Loss 1.654967]\n",
            "Iter 20. [Val Acc 63%, Loss 0.970947]\n",
            "Iter 30. [Val Acc 65%, Loss 0.823124]\n",
            "Iter 40. [Val Acc 67%, Loss 0.757850]\n",
            "Iter 50. [Val Acc 68%, Loss 0.701351]\n",
            "Iter 60. [Val Acc 69%, Loss 0.653872]\n",
            "Iter 70. [Val Acc 69%, Loss 0.643421]\n",
            "Iter 80. [Val Acc 65%, Loss 0.703670]\n",
            "Iter 90. [Val Acc 72%, Loss 0.616300]\n",
            "Iter 100. [Val Acc 67%, Loss 0.651405]\n",
            "Iter 10. [Val Acc 57%, Loss 1.670037]\n",
            "Iter 20. [Val Acc 62%, Loss 1.000269]\n",
            "Iter 30. [Val Acc 65%, Loss 0.818260]\n",
            "Iter 40. [Val Acc 67%, Loss 0.728219]\n",
            "Iter 50. [Val Acc 68%, Loss 0.687103]\n",
            "Iter 60. [Val Acc 65%, Loss 0.745710]\n",
            "Iter 70. [Val Acc 69%, Loss 0.630326]\n",
            "Iter 80. [Val Acc 68%, Loss 0.640484]\n",
            "Iter 90. [Val Acc 68%, Loss 0.653542]\n",
            "Iter 100. [Val Acc 70%, Loss 0.616905]\n",
            "Iter 10. [Val Acc 55%, Loss 1.612830]\n",
            "Iter 20. [Val Acc 63%, Loss 0.985476]\n",
            "Iter 30. [Val Acc 66%, Loss 0.788591]\n",
            "Iter 40. [Val Acc 67%, Loss 0.763072]\n",
            "Iter 50. [Val Acc 70%, Loss 0.661082]\n",
            "Iter 60. [Val Acc 69%, Loss 0.658532]\n",
            "Iter 70. [Val Acc 69%, Loss 0.639128]\n",
            "Iter 80. [Val Acc 70%, Loss 0.623744]\n",
            "Iter 90. [Val Acc 69%, Loss 0.653633]\n",
            "Iter 100. [Val Acc 70%, Loss 0.613321]\n",
            "Iter 10. [Val Acc 56%, Loss 1.669687]\n",
            "Iter 20. [Val Acc 64%, Loss 0.967542]\n",
            "Iter 30. [Val Acc 66%, Loss 0.774293]\n",
            "Iter 40. [Val Acc 68%, Loss 0.684989]\n",
            "Iter 50. [Val Acc 69%, Loss 0.654303]\n",
            "Iter 60. [Val Acc 68%, Loss 0.654022]\n",
            "Iter 70. [Val Acc 68%, Loss 0.683678]\n",
            "Iter 80. [Val Acc 70%, Loss 0.627312]\n",
            "Iter 90. [Val Acc 71%, Loss 0.612998]\n",
            "Iter 100. [Val Acc 70%, Loss 0.637058]\n",
            "Iter 10. [Val Acc 57%, Loss 1.630453]\n",
            "Iter 20. [Val Acc 63%, Loss 0.983728]\n",
            "Iter 30. [Val Acc 66%, Loss 0.773206]\n",
            "Iter 40. [Val Acc 68%, Loss 0.689712]\n",
            "Iter 50. [Val Acc 69%, Loss 0.652530]\n",
            "Iter 60. [Val Acc 68%, Loss 0.678815]\n",
            "Iter 70. [Val Acc 69%, Loss 0.632019]\n",
            "Iter 80. [Val Acc 70%, Loss 0.618469]\n",
            "Iter 90. [Val Acc 70%, Loss 0.644079]\n",
            "Iter 100. [Val Acc 68%, Loss 0.663777]\n",
            "Iter 10. [Val Acc 57%, Loss 1.709384]\n",
            "Iter 20. [Val Acc 63%, Loss 1.312265]\n",
            "Iter 30. [Val Acc 65%, Loss 1.033082]\n",
            "Iter 40. [Val Acc 64%, Loss 1.066421]\n",
            "Iter 50. [Val Acc 63%, Loss 1.189613]\n",
            "Iter 60. [Val Acc 62%, Loss 1.174657]\n",
            "Iter 70. [Val Acc 67%, Loss 0.900047]\n",
            "Iter 80. [Val Acc 64%, Loss 1.089028]\n",
            "Iter 90. [Val Acc 66%, Loss 0.990022]\n",
            "Iter 100. [Val Acc 65%, Loss 1.063934]\n",
            "Iter 10. [Val Acc 58%, Loss 1.616821]\n",
            "Iter 20. [Val Acc 65%, Loss 0.963204]\n",
            "Iter 30. [Val Acc 66%, Loss 0.899183]\n",
            "Iter 40. [Val Acc 64%, Loss 0.932113]\n",
            "Iter 50. [Val Acc 68%, Loss 0.824419]\n",
            "Iter 60. [Val Acc 64%, Loss 0.956760]\n",
            "Iter 70. [Val Acc 67%, Loss 0.792239]\n",
            "Iter 80. [Val Acc 69%, Loss 0.766150]\n",
            "Iter 90. [Val Acc 64%, Loss 0.867615]\n",
            "Iter 100. [Val Acc 66%, Loss 0.792231]\n",
            "Iter 10. [Val Acc 58%, Loss 1.427549]\n",
            "Iter 20. [Val Acc 64%, Loss 0.940531]\n",
            "Iter 30. [Val Acc 66%, Loss 0.876704]\n",
            "Iter 40. [Val Acc 65%, Loss 0.810030]\n",
            "Iter 50. [Val Acc 66%, Loss 0.871506]\n",
            "Iter 60. [Val Acc 69%, Loss 0.712755]\n",
            "Iter 70. [Val Acc 66%, Loss 0.739404]\n",
            "Iter 80. [Val Acc 66%, Loss 0.890937]\n",
            "Iter 90. [Val Acc 67%, Loss 0.791824]\n",
            "Iter 100. [Val Acc 58%, Loss 1.137802]\n",
            "Iter 10. [Val Acc 58%, Loss 1.441350]\n",
            "Iter 20. [Val Acc 64%, Loss 0.945177]\n",
            "Iter 30. [Val Acc 63%, Loss 0.979934]\n",
            "Iter 40. [Val Acc 70%, Loss 0.698323]\n",
            "Iter 50. [Val Acc 65%, Loss 0.775932]\n",
            "Iter 60. [Val Acc 66%, Loss 0.744356]\n",
            "Iter 70. [Val Acc 69%, Loss 0.715110]\n",
            "Iter 80. [Val Acc 67%, Loss 0.679499]\n",
            "Iter 90. [Val Acc 68%, Loss 0.738761]\n",
            "Iter 100. [Val Acc 62%, Loss 0.930925]\n",
            "Iter 10. [Val Acc 58%, Loss 1.405919]\n",
            "Iter 20. [Val Acc 63%, Loss 0.937714]\n",
            "Iter 30. [Val Acc 67%, Loss 0.775212]\n",
            "Iter 40. [Val Acc 65%, Loss 0.784789]\n",
            "Iter 50. [Val Acc 68%, Loss 0.690271]\n",
            "Iter 60. [Val Acc 68%, Loss 0.700321]\n",
            "Iter 70. [Val Acc 62%, Loss 0.847762]\n",
            "Iter 80. [Val Acc 69%, Loss 0.699880]\n",
            "Iter 90. [Val Acc 66%, Loss 0.841765]\n",
            "Iter 100. [Val Acc 69%, Loss 0.656206]\n",
            "Iter 10. [Val Acc 57%, Loss 1.501024]\n",
            "Iter 20. [Val Acc 65%, Loss 0.865473]\n",
            "Iter 30. [Val Acc 67%, Loss 0.772193]\n",
            "Iter 40. [Val Acc 67%, Loss 0.766131]\n",
            "Iter 50. [Val Acc 68%, Loss 0.649783]\n",
            "Iter 60. [Val Acc 66%, Loss 0.719411]\n",
            "Iter 70. [Val Acc 58%, Loss 1.093309]\n",
            "Iter 80. [Val Acc 70%, Loss 0.647225]\n",
            "Iter 90. [Val Acc 69%, Loss 0.634507]\n",
            "Iter 100. [Val Acc 66%, Loss 0.807194]\n",
            "Iter 10. [Val Acc 59%, Loss 1.420082]\n",
            "Iter 20. [Val Acc 65%, Loss 0.879684]\n",
            "Iter 30. [Val Acc 63%, Loss 0.927962]\n",
            "Iter 40. [Val Acc 65%, Loss 0.805714]\n",
            "Iter 50. [Val Acc 69%, Loss 0.688722]\n",
            "Iter 60. [Val Acc 60%, Loss 1.049987]\n",
            "Iter 70. [Val Acc 63%, Loss 0.845131]\n",
            "Iter 80. [Val Acc 71%, Loss 0.639059]\n",
            "Iter 90. [Val Acc 66%, Loss 0.764659]\n",
            "Iter 100. [Val Acc 69%, Loss 0.664629]\n",
            "Iter 10. [Val Acc 60%, Loss 1.414534]\n",
            "Iter 20. [Val Acc 65%, Loss 0.897239]\n",
            "Iter 30. [Val Acc 66%, Loss 0.738610]\n",
            "Iter 40. [Val Acc 65%, Loss 0.761483]\n",
            "Iter 50. [Val Acc 61%, Loss 0.947772]\n",
            "Iter 60. [Val Acc 69%, Loss 0.635708]\n",
            "Iter 70. [Val Acc 66%, Loss 0.851329]\n",
            "Iter 80. [Val Acc 65%, Loss 0.877332]\n",
            "Iter 90. [Val Acc 67%, Loss 0.682761]\n",
            "Iter 100. [Val Acc 67%, Loss 0.726875]\n",
            "Iter 10. [Val Acc 59%, Loss 1.395452]\n",
            "Iter 20. [Val Acc 65%, Loss 0.858808]\n",
            "Iter 30. [Val Acc 68%, Loss 0.723554]\n",
            "Iter 40. [Val Acc 63%, Loss 0.839394]\n",
            "Iter 50. [Val Acc 65%, Loss 0.781867]\n",
            "Iter 60. [Val Acc 69%, Loss 0.663878]\n",
            "Iter 70. [Val Acc 65%, Loss 0.769122]\n",
            "Iter 80. [Val Acc 62%, Loss 0.909617]\n",
            "Iter 90. [Val Acc 63%, Loss 0.804371]\n",
            "Iter 100. [Val Acc 65%, Loss 0.805893]\n",
            "Iter 10. [Val Acc 58%, Loss 1.670046]\n",
            "Iter 20. [Val Acc 62%, Loss 1.283831]\n",
            "Iter 30. [Val Acc 64%, Loss 1.040793]\n",
            "Iter 40. [Val Acc 66%, Loss 1.291576]\n",
            "Iter 50. [Val Acc 64%, Loss 1.013299]\n",
            "Iter 60. [Val Acc 64%, Loss 1.308351]\n",
            "Iter 70. [Val Acc 60%, Loss 1.407769]\n",
            "Iter 80. [Val Acc 64%, Loss 1.146779]\n",
            "Iter 90. [Val Acc 65%, Loss 1.180131]\n",
            "Iter 100. [Val Acc 65%, Loss 1.062506]\n",
            "Iter 10. [Val Acc 57%, Loss 1.516847]\n",
            "Iter 20. [Val Acc 63%, Loss 1.248765]\n",
            "Iter 30. [Val Acc 63%, Loss 0.988535]\n",
            "Iter 40. [Val Acc 58%, Loss 1.443398]\n",
            "Iter 50. [Val Acc 59%, Loss 1.259836]\n",
            "Iter 60. [Val Acc 66%, Loss 0.932406]\n",
            "Iter 70. [Val Acc 63%, Loss 1.004211]\n",
            "Iter 80. [Val Acc 68%, Loss 0.829894]\n",
            "Iter 90. [Val Acc 64%, Loss 1.056307]\n",
            "Iter 100. [Val Acc 67%, Loss 0.936426]\n",
            "Iter 10. [Val Acc 57%, Loss 1.487484]\n",
            "Iter 20. [Val Acc 66%, Loss 0.971081]\n",
            "Iter 30. [Val Acc 65%, Loss 0.875446]\n",
            "Iter 40. [Val Acc 66%, Loss 0.878170]\n",
            "Iter 50. [Val Acc 65%, Loss 0.925741]\n",
            "Iter 60. [Val Acc 68%, Loss 0.835531]\n",
            "Iter 70. [Val Acc 66%, Loss 0.805187]\n",
            "Iter 80. [Val Acc 58%, Loss 1.664721]\n",
            "Iter 90. [Val Acc 68%, Loss 0.761115]\n",
            "Iter 100. [Val Acc 66%, Loss 0.793313]\n",
            "Iter 10. [Val Acc 61%, Loss 1.367377]\n",
            "Iter 20. [Val Acc 62%, Loss 1.170629]\n",
            "Iter 30. [Val Acc 64%, Loss 0.878378]\n",
            "Iter 40. [Val Acc 67%, Loss 0.834219]\n",
            "Iter 50. [Val Acc 69%, Loss 0.708939]\n",
            "Iter 60. [Val Acc 64%, Loss 1.010479]\n",
            "Iter 70. [Val Acc 67%, Loss 0.765820]\n",
            "Iter 80. [Val Acc 69%, Loss 0.764913]\n",
            "Iter 90. [Val Acc 59%, Loss 1.885621]\n",
            "Iter 100. [Val Acc 63%, Loss 0.950099]\n",
            "Iter 10. [Val Acc 60%, Loss 1.292327]\n",
            "Iter 20. [Val Acc 60%, Loss 1.159518]\n",
            "Iter 30. [Val Acc 68%, Loss 0.751567]\n",
            "Iter 40. [Val Acc 64%, Loss 1.057143]\n",
            "Iter 50. [Val Acc 70%, Loss 0.706413]\n",
            "Iter 60. [Val Acc 65%, Loss 0.961432]\n",
            "Iter 70. [Val Acc 68%, Loss 0.799574]\n",
            "Iter 80. [Val Acc 64%, Loss 0.886909]\n",
            "Iter 90. [Val Acc 67%, Loss 0.800710]\n",
            "Iter 100. [Val Acc 58%, Loss 1.296564]\n",
            "Iter 10. [Val Acc 61%, Loss 1.236130]\n",
            "Iter 20. [Val Acc 60%, Loss 1.129350]\n",
            "Iter 30. [Val Acc 68%, Loss 0.793611]\n",
            "Iter 40. [Val Acc 66%, Loss 0.856750]\n",
            "Iter 50. [Val Acc 66%, Loss 0.888776]\n",
            "Iter 60. [Val Acc 62%, Loss 1.369430]\n",
            "Iter 70. [Val Acc 61%, Loss 1.071478]\n",
            "Iter 80. [Val Acc 66%, Loss 0.812434]\n",
            "Iter 90. [Val Acc 63%, Loss 1.034219]\n",
            "Iter 100. [Val Acc 63%, Loss 1.000259]\n",
            "Iter 10. [Val Acc 56%, Loss 1.420670]\n",
            "Iter 20. [Val Acc 66%, Loss 0.859217]\n",
            "Iter 30. [Val Acc 62%, Loss 1.047057]\n",
            "Iter 40. [Val Acc 60%, Loss 1.050655]\n",
            "Iter 50. [Val Acc 65%, Loss 0.869107]\n",
            "Iter 60. [Val Acc 67%, Loss 0.800237]\n",
            "Iter 70. [Val Acc 64%, Loss 0.877277]\n",
            "Iter 80. [Val Acc 63%, Loss 0.935999]\n",
            "Iter 90. [Val Acc 64%, Loss 0.892923]\n",
            "Iter 100. [Val Acc 69%, Loss 0.798917]\n",
            "Iter 10. [Val Acc 60%, Loss 1.273217]\n",
            "Iter 20. [Val Acc 67%, Loss 0.784166]\n",
            "Iter 30. [Val Acc 59%, Loss 1.028764]\n",
            "Iter 40. [Val Acc 61%, Loss 1.056078]\n",
            "Iter 50. [Val Acc 56%, Loss 1.504486]\n",
            "Iter 60. [Val Acc 66%, Loss 0.839550]\n",
            "Iter 70. [Val Acc 68%, Loss 0.806847]\n",
            "Iter 80. [Val Acc 62%, Loss 1.028793]\n",
            "Iter 90. [Val Acc 66%, Loss 0.823830]\n",
            "Iter 100. [Val Acc 66%, Loss 0.862710]\n",
            "Iter 10. [Val Acc 60%, Loss 1.243545]\n",
            "Iter 20. [Val Acc 63%, Loss 0.853121]\n",
            "Iter 30. [Val Acc 62%, Loss 0.941327]\n",
            "Iter 40. [Val Acc 64%, Loss 0.903999]\n",
            "Iter 50. [Val Acc 64%, Loss 0.917024]\n",
            "Iter 60. [Val Acc 66%, Loss 0.799307]\n",
            "Iter 70. [Val Acc 60%, Loss 1.155799]\n",
            "Iter 80. [Val Acc 60%, Loss 1.199982]\n",
            "Iter 90. [Val Acc 61%, Loss 1.117624]\n",
            "Iter 100. [Val Acc 66%, Loss 0.838043]\n",
            "Iter 10. [Val Acc 60%, Loss 1.538257]\n",
            "Iter 20. [Val Acc 61%, Loss 1.641943]\n",
            "Iter 30. [Val Acc 64%, Loss 1.184291]\n",
            "Iter 40. [Val Acc 60%, Loss 1.366172]\n",
            "Iter 50. [Val Acc 57%, Loss 1.725524]\n",
            "Iter 60. [Val Acc 64%, Loss 1.638851]\n",
            "Iter 70. [Val Acc 59%, Loss 1.872299]\n",
            "Iter 80. [Val Acc 62%, Loss 1.377726]\n",
            "Iter 90. [Val Acc 64%, Loss 1.295912]\n",
            "Iter 100. [Val Acc 66%, Loss 1.203838]\n",
            "Iter 10. [Val Acc 58%, Loss 1.651292]\n",
            "Iter 20. [Val Acc 60%, Loss 1.355954]\n",
            "Iter 30. [Val Acc 57%, Loss 1.729783]\n",
            "Iter 40. [Val Acc 54%, Loss 1.899589]\n",
            "Iter 50. [Val Acc 69%, Loss 0.923071]\n",
            "Iter 60. [Val Acc 68%, Loss 1.004612]\n",
            "Iter 70. [Val Acc 65%, Loss 1.103738]\n",
            "Iter 80. [Val Acc 68%, Loss 0.982009]\n",
            "Iter 90. [Val Acc 58%, Loss 1.745423]\n",
            "Iter 100. [Val Acc 60%, Loss 1.184646]\n",
            "Iter 10. [Val Acc 62%, Loss 1.333001]\n",
            "Iter 20. [Val Acc 64%, Loss 1.032193]\n",
            "Iter 30. [Val Acc 63%, Loss 1.040828]\n",
            "Iter 40. [Val Acc 66%, Loss 0.885128]\n",
            "Iter 50. [Val Acc 64%, Loss 1.051008]\n",
            "Iter 60. [Val Acc 64%, Loss 0.996141]\n",
            "Iter 70. [Val Acc 63%, Loss 1.288999]\n",
            "Iter 80. [Val Acc 67%, Loss 0.878628]\n",
            "Iter 90. [Val Acc 66%, Loss 0.950166]\n",
            "Iter 100. [Val Acc 65%, Loss 1.034064]\n",
            "Iter 10. [Val Acc 60%, Loss 1.176846]\n",
            "Iter 20. [Val Acc 66%, Loss 0.910706]\n",
            "Iter 30. [Val Acc 58%, Loss 1.245050]\n",
            "Iter 40. [Val Acc 57%, Loss 1.626630]\n",
            "Iter 50. [Val Acc 60%, Loss 1.120331]\n",
            "Iter 60. [Val Acc 67%, Loss 0.815816]\n",
            "Iter 70. [Val Acc 68%, Loss 0.786766]\n",
            "Iter 80. [Val Acc 67%, Loss 0.892941]\n",
            "Iter 90. [Val Acc 60%, Loss 1.303801]\n",
            "Iter 100. [Val Acc 61%, Loss 1.028387]\n",
            "Iter 10. [Val Acc 62%, Loss 1.168766]\n",
            "Iter 20. [Val Acc 67%, Loss 0.910160]\n",
            "Iter 30. [Val Acc 66%, Loss 0.909100]\n",
            "Iter 40. [Val Acc 66%, Loss 0.925203]\n",
            "Iter 50. [Val Acc 65%, Loss 0.974993]\n",
            "Iter 60. [Val Acc 65%, Loss 1.107374]\n",
            "Iter 70. [Val Acc 65%, Loss 1.093679]\n",
            "Iter 80. [Val Acc 67%, Loss 0.888550]\n",
            "Iter 90. [Val Acc 66%, Loss 0.907173]\n",
            "Iter 100. [Val Acc 65%, Loss 0.997524]\n",
            "Iter 10. [Val Acc 62%, Loss 1.285181]\n",
            "Iter 20. [Val Acc 60%, Loss 1.449050]\n",
            "Iter 30. [Val Acc 62%, Loss 1.219940]\n",
            "Iter 40. [Val Acc 65%, Loss 1.146756]\n",
            "Iter 50. [Val Acc 64%, Loss 0.967072]\n",
            "Iter 60. [Val Acc 66%, Loss 1.152008]\n",
            "Iter 70. [Val Acc 69%, Loss 0.835601]\n",
            "Iter 80. [Val Acc 64%, Loss 0.983888]\n",
            "Iter 90. [Val Acc 64%, Loss 1.034823]\n",
            "Iter 100. [Val Acc 63%, Loss 1.027695]\n",
            "Iter 10. [Val Acc 61%, Loss 1.183828]\n",
            "Iter 20. [Val Acc 60%, Loss 1.373518]\n",
            "Iter 30. [Val Acc 67%, Loss 0.914408]\n",
            "Iter 40. [Val Acc 66%, Loss 0.909948]\n",
            "Iter 50. [Val Acc 62%, Loss 1.138475]\n",
            "Iter 60. [Val Acc 60%, Loss 1.243490]\n",
            "Iter 70. [Val Acc 62%, Loss 1.174336]\n",
            "Iter 80. [Val Acc 62%, Loss 1.128269]\n",
            "Iter 90. [Val Acc 61%, Loss 1.141622]\n",
            "Iter 100. [Val Acc 69%, Loss 0.850676]\n",
            "Iter 10. [Val Acc 62%, Loss 1.152696]\n",
            "Iter 20. [Val Acc 65%, Loss 0.920019]\n",
            "Iter 30. [Val Acc 60%, Loss 1.274987]\n",
            "Iter 40. [Val Acc 68%, Loss 0.789483]\n",
            "Iter 50. [Val Acc 66%, Loss 0.934982]\n",
            "Iter 60. [Val Acc 64%, Loss 0.948881]\n",
            "Iter 70. [Val Acc 63%, Loss 1.260481]\n",
            "Iter 80. [Val Acc 64%, Loss 1.462295]\n",
            "Iter 90. [Val Acc 64%, Loss 1.118446]\n",
            "Iter 100. [Val Acc 65%, Loss 1.131885]\n",
            "Iter 10. [Val Acc 59%, Loss 1.173109]\n",
            "Iter 20. [Val Acc 61%, Loss 1.236773]\n",
            "Iter 30. [Val Acc 62%, Loss 1.122911]\n",
            "Iter 40. [Val Acc 66%, Loss 0.914406]\n",
            "Iter 50. [Val Acc 66%, Loss 0.915306]\n",
            "Iter 60. [Val Acc 66%, Loss 0.970818]\n",
            "Iter 70. [Val Acc 62%, Loss 1.083463]\n",
            "Iter 80. [Val Acc 62%, Loss 1.115490]\n",
            "Iter 90. [Val Acc 62%, Loss 1.052190]\n",
            "Iter 100. [Val Acc 62%, Loss 1.090076]\n",
            "Iter 10. [Val Acc 54%, Loss 2.397364]\n",
            "Iter 20. [Val Acc 63%, Loss 1.783386]\n",
            "Iter 30. [Val Acc 65%, Loss 1.426430]\n",
            "Iter 40. [Val Acc 62%, Loss 1.750458]\n",
            "Iter 50. [Val Acc 66%, Loss 1.408358]\n",
            "Iter 60. [Val Acc 64%, Loss 1.504031]\n",
            "Iter 70. [Val Acc 64%, Loss 1.164767]\n",
            "Iter 80. [Val Acc 64%, Loss 1.311977]\n",
            "Iter 90. [Val Acc 61%, Loss 2.159632]\n",
            "Iter 100. [Val Acc 66%, Loss 1.185731]\n",
            "Iter 10. [Val Acc 60%, Loss 1.590852]\n",
            "Iter 20. [Val Acc 65%, Loss 0.972996]\n",
            "Iter 30. [Val Acc 64%, Loss 1.488728]\n",
            "Iter 40. [Val Acc 60%, Loss 1.287742]\n",
            "Iter 50. [Val Acc 64%, Loss 1.095646]\n",
            "Iter 60. [Val Acc 67%, Loss 0.876148]\n",
            "Iter 70. [Val Acc 59%, Loss 1.492281]\n",
            "Iter 80. [Val Acc 65%, Loss 1.094485]\n",
            "Iter 90. [Val Acc 60%, Loss 1.435073]\n",
            "Iter 100. [Val Acc 63%, Loss 1.486273]\n",
            "Iter 10. [Val Acc 58%, Loss 1.440374]\n",
            "Iter 20. [Val Acc 63%, Loss 1.209987]\n",
            "Iter 30. [Val Acc 64%, Loss 1.395088]\n",
            "Iter 40. [Val Acc 67%, Loss 0.988214]\n",
            "Iter 50. [Val Acc 65%, Loss 1.288441]\n",
            "Iter 60. [Val Acc 67%, Loss 0.945415]\n",
            "Iter 70. [Val Acc 64%, Loss 1.332898]\n",
            "Iter 80. [Val Acc 68%, Loss 1.055783]\n",
            "Iter 90. [Val Acc 56%, Loss 2.187308]\n",
            "Iter 100. [Val Acc 67%, Loss 0.950767]\n",
            "Iter 10. [Val Acc 61%, Loss 1.304288]\n",
            "Iter 20. [Val Acc 64%, Loss 1.272901]\n",
            "Iter 30. [Val Acc 66%, Loss 1.109463]\n",
            "Iter 40. [Val Acc 65%, Loss 1.223309]\n",
            "Iter 50. [Val Acc 60%, Loss 1.174755]\n",
            "Iter 60. [Val Acc 57%, Loss 1.708535]\n",
            "Iter 70. [Val Acc 62%, Loss 1.263936]\n",
            "Iter 80. [Val Acc 68%, Loss 0.964906]\n",
            "Iter 90. [Val Acc 66%, Loss 0.938779]\n",
            "Iter 100. [Val Acc 64%, Loss 1.299881]\n",
            "Iter 10. [Val Acc 62%, Loss 1.301352]\n",
            "Iter 20. [Val Acc 65%, Loss 1.004449]\n",
            "Iter 30. [Val Acc 64%, Loss 1.347083]\n",
            "Iter 40. [Val Acc 63%, Loss 1.194021]\n",
            "Iter 50. [Val Acc 62%, Loss 1.746973]\n",
            "Iter 60. [Val Acc 66%, Loss 1.195664]\n",
            "Iter 70. [Val Acc 66%, Loss 1.042346]\n",
            "Iter 80. [Val Acc 61%, Loss 1.610978]\n",
            "Iter 90. [Val Acc 59%, Loss 1.247560]\n",
            "Iter 100. [Val Acc 59%, Loss 1.481120]\n",
            "Iter 10. [Val Acc 57%, Loss 1.533235]\n",
            "Iter 20. [Val Acc 62%, Loss 1.182829]\n",
            "Iter 30. [Val Acc 62%, Loss 1.386900]\n",
            "Iter 40. [Val Acc 68%, Loss 0.853626]\n",
            "Iter 50. [Val Acc 61%, Loss 1.037435]\n",
            "Iter 60. [Val Acc 60%, Loss 1.370238]\n",
            "Iter 70. [Val Acc 62%, Loss 1.238244]\n",
            "Iter 80. [Val Acc 68%, Loss 0.960247]\n",
            "Iter 90. [Val Acc 57%, Loss 1.750041]\n",
            "Iter 100. [Val Acc 65%, Loss 0.908064]\n",
            "Iter 10. [Val Acc 62%, Loss 1.066133]\n",
            "Iter 20. [Val Acc 57%, Loss 1.567985]\n",
            "Iter 30. [Val Acc 58%, Loss 1.662157]\n",
            "Iter 40. [Val Acc 60%, Loss 1.432589]\n",
            "Iter 50. [Val Acc 65%, Loss 1.065402]\n",
            "Iter 60. [Val Acc 56%, Loss 2.163712]\n",
            "Iter 70. [Val Acc 59%, Loss 1.573139]\n",
            "Iter 80. [Val Acc 58%, Loss 1.476340]\n",
            "Iter 90. [Val Acc 58%, Loss 1.747313]\n",
            "Iter 100. [Val Acc 67%, Loss 0.956221]\n",
            "Iter 10. [Val Acc 60%, Loss 1.498096]\n",
            "Iter 20. [Val Acc 64%, Loss 1.288980]\n",
            "Iter 30. [Val Acc 64%, Loss 1.332790]\n",
            "Iter 40. [Val Acc 67%, Loss 0.986666]\n",
            "Iter 50. [Val Acc 65%, Loss 1.189524]\n",
            "Iter 60. [Val Acc 66%, Loss 1.277792]\n",
            "Iter 70. [Val Acc 65%, Loss 1.259492]\n",
            "Iter 80. [Val Acc 65%, Loss 1.271234]\n",
            "Iter 90. [Val Acc 62%, Loss 1.401372]\n",
            "Iter 100. [Val Acc 64%, Loss 1.352977]\n",
            "Iter 10. [Val Acc 61%, Loss 1.165547]\n",
            "Iter 20. [Val Acc 66%, Loss 0.904673]\n",
            "Iter 30. [Val Acc 62%, Loss 1.207779]\n",
            "Iter 40. [Val Acc 60%, Loss 1.399184]\n",
            "Iter 50. [Val Acc 62%, Loss 1.148109]\n",
            "Iter 60. [Val Acc 57%, Loss 1.718761]\n",
            "Iter 70. [Val Acc 61%, Loss 1.357369]\n",
            "Iter 80. [Val Acc 63%, Loss 1.049112]\n",
            "Iter 90. [Val Acc 64%, Loss 1.049764]\n",
            "Iter 100. [Val Acc 61%, Loss 1.359862]\n",
            "Iter 10. [Val Acc 61%, Loss 1.751770]\n",
            "Iter 20. [Val Acc 61%, Loss 1.956655]\n",
            "Iter 30. [Val Acc 65%, Loss 1.301160]\n",
            "Iter 40. [Val Acc 64%, Loss 1.545989]\n",
            "Iter 50. [Val Acc 64%, Loss 1.855605]\n",
            "Iter 60. [Val Acc 63%, Loss 1.701642]\n",
            "Iter 70. [Val Acc 63%, Loss 1.804088]\n",
            "Iter 80. [Val Acc 61%, Loss 2.279703]\n",
            "Iter 90. [Val Acc 65%, Loss 1.715019]\n",
            "Iter 100. [Val Acc 61%, Loss 2.158442]\n",
            "Iter 10. [Val Acc 62%, Loss 1.436454]\n",
            "Iter 20. [Val Acc 59%, Loss 2.573043]\n",
            "Iter 30. [Val Acc 61%, Loss 1.572306]\n",
            "Iter 40. [Val Acc 59%, Loss 1.637616]\n",
            "Iter 50. [Val Acc 66%, Loss 1.145269]\n",
            "Iter 60. [Val Acc 59%, Loss 1.527787]\n",
            "Iter 70. [Val Acc 66%, Loss 1.381714]\n",
            "Iter 80. [Val Acc 60%, Loss 1.330222]\n",
            "Iter 90. [Val Acc 65%, Loss 1.391770]\n",
            "Iter 100. [Val Acc 66%, Loss 1.201990]\n",
            "Iter 10. [Val Acc 63%, Loss 1.660018]\n",
            "Iter 20. [Val Acc 64%, Loss 1.132604]\n",
            "Iter 30. [Val Acc 56%, Loss 1.779558]\n",
            "Iter 40. [Val Acc 55%, Loss 2.050253]\n",
            "Iter 50. [Val Acc 65%, Loss 1.038069]\n",
            "Iter 60. [Val Acc 55%, Loss 2.338262]\n",
            "Iter 70. [Val Acc 69%, Loss 0.950976]\n",
            "Iter 80. [Val Acc 63%, Loss 1.430513]\n",
            "Iter 90. [Val Acc 64%, Loss 1.732877]\n",
            "Iter 100. [Val Acc 64%, Loss 1.572074]\n",
            "Iter 10. [Val Acc 61%, Loss 1.333508]\n",
            "Iter 20. [Val Acc 63%, Loss 1.403048]\n",
            "Iter 30. [Val Acc 63%, Loss 1.433919]\n",
            "Iter 40. [Val Acc 65%, Loss 1.289604]\n",
            "Iter 50. [Val Acc 64%, Loss 1.228822]\n",
            "Iter 60. [Val Acc 62%, Loss 1.602972]\n",
            "Iter 70. [Val Acc 60%, Loss 2.119718]\n",
            "Iter 80. [Val Acc 63%, Loss 1.348493]\n",
            "Iter 90. [Val Acc 63%, Loss 1.318797]\n",
            "Iter 100. [Val Acc 55%, Loss 2.303028]\n",
            "Iter 10. [Val Acc 59%, Loss 1.503974]\n",
            "Iter 20. [Val Acc 58%, Loss 1.677796]\n",
            "Iter 30. [Val Acc 60%, Loss 1.295636]\n",
            "Iter 40. [Val Acc 68%, Loss 0.904777]\n",
            "Iter 50. [Val Acc 66%, Loss 1.055482]\n",
            "Iter 60. [Val Acc 60%, Loss 2.145502]\n",
            "Iter 70. [Val Acc 63%, Loss 1.671053]\n",
            "Iter 80. [Val Acc 64%, Loss 1.478721]\n",
            "Iter 90. [Val Acc 65%, Loss 1.167767]\n",
            "Iter 100. [Val Acc 58%, Loss 2.446625]\n",
            "Iter 10. [Val Acc 62%, Loss 1.270184]\n",
            "Iter 20. [Val Acc 65%, Loss 1.399391]\n",
            "Iter 30. [Val Acc 64%, Loss 1.488151]\n",
            "Iter 40. [Val Acc 63%, Loss 1.837847]\n",
            "Iter 50. [Val Acc 66%, Loss 1.362885]\n",
            "Iter 60. [Val Acc 63%, Loss 1.876508]\n",
            "Iter 70. [Val Acc 63%, Loss 1.672766]\n",
            "Iter 80. [Val Acc 62%, Loss 1.499103]\n",
            "Iter 90. [Val Acc 60%, Loss 1.696884]\n",
            "Iter 100. [Val Acc 61%, Loss 1.499286]\n",
            "Iter 10. [Val Acc 60%, Loss 1.815126]\n",
            "Iter 20. [Val Acc 63%, Loss 1.528486]\n",
            "Iter 30. [Val Acc 63%, Loss 1.675991]\n",
            "Iter 40. [Val Acc 66%, Loss 1.120280]\n",
            "Iter 50. [Val Acc 68%, Loss 0.924477]\n",
            "Iter 60. [Val Acc 66%, Loss 1.242672]\n",
            "Iter 70. [Val Acc 62%, Loss 1.829988]\n",
            "Iter 80. [Val Acc 66%, Loss 1.213882]\n",
            "Iter 90. [Val Acc 66%, Loss 1.196804]\n",
            "Iter 100. [Val Acc 63%, Loss 1.617650]\n",
            "Iter 10. [Val Acc 61%, Loss 1.665710]\n",
            "Iter 20. [Val Acc 64%, Loss 1.617051]\n",
            "Iter 30. [Val Acc 63%, Loss 1.772438]\n",
            "Iter 40. [Val Acc 64%, Loss 1.322112]\n",
            "Iter 50. [Val Acc 66%, Loss 1.100105]\n",
            "Iter 60. [Val Acc 64%, Loss 1.347536]\n",
            "Iter 70. [Val Acc 63%, Loss 1.722786]\n",
            "Iter 80. [Val Acc 64%, Loss 1.722334]\n",
            "Iter 90. [Val Acc 66%, Loss 1.245313]\n",
            "Iter 100. [Val Acc 64%, Loss 1.446790]\n",
            "Iter 10. [Val Acc 59%, Loss 1.564877]\n",
            "Iter 20. [Val Acc 59%, Loss 1.679845]\n",
            "Iter 30. [Val Acc 66%, Loss 1.089931]\n",
            "Iter 40. [Val Acc 63%, Loss 1.322978]\n",
            "Iter 50. [Val Acc 61%, Loss 1.482012]\n",
            "Iter 60. [Val Acc 61%, Loss 1.748887]\n",
            "Iter 70. [Val Acc 61%, Loss 1.466308]\n",
            "Iter 80. [Val Acc 62%, Loss 1.454772]\n",
            "Iter 90. [Val Acc 64%, Loss 1.216029]\n",
            "Iter 100. [Val Acc 62%, Loss 1.378721]\n",
            "Iter 10. [Val Acc 58%, Loss 2.540087]\n",
            "Iter 20. [Val Acc 63%, Loss 1.678517]\n",
            "Iter 30. [Val Acc 63%, Loss 1.931414]\n",
            "Iter 40. [Val Acc 65%, Loss 1.429407]\n",
            "Iter 50. [Val Acc 62%, Loss 1.920302]\n",
            "Iter 60. [Val Acc 60%, Loss 1.998163]\n",
            "Iter 70. [Val Acc 60%, Loss 2.286795]\n",
            "Iter 80. [Val Acc 62%, Loss 1.850553]\n",
            "Iter 90. [Val Acc 63%, Loss 2.342472]\n",
            "Iter 100. [Val Acc 67%, Loss 1.714129]\n",
            "Iter 10. [Val Acc 61%, Loss 1.677167]\n",
            "Iter 20. [Val Acc 63%, Loss 1.629433]\n",
            "Iter 30. [Val Acc 64%, Loss 1.822450]\n",
            "Iter 40. [Val Acc 60%, Loss 1.550352]\n",
            "Iter 50. [Val Acc 68%, Loss 1.295353]\n",
            "Iter 60. [Val Acc 58%, Loss 2.044841]\n",
            "Iter 70. [Val Acc 56%, Loss 2.375122]\n",
            "Iter 80. [Val Acc 66%, Loss 1.098670]\n",
            "Iter 90. [Val Acc 61%, Loss 1.358050]\n",
            "Iter 100. [Val Acc 55%, Loss 2.485773]\n",
            "Iter 10. [Val Acc 58%, Loss 1.493289]\n",
            "Iter 20. [Val Acc 64%, Loss 1.319577]\n",
            "Iter 30. [Val Acc 61%, Loss 2.399483]\n",
            "Iter 40. [Val Acc 68%, Loss 1.143636]\n",
            "Iter 50. [Val Acc 64%, Loss 1.723741]\n",
            "Iter 60. [Val Acc 65%, Loss 1.484166]\n",
            "Iter 70. [Val Acc 63%, Loss 1.686411]\n",
            "Iter 80. [Val Acc 67%, Loss 1.114909]\n",
            "Iter 90. [Val Acc 62%, Loss 1.695853]\n",
            "Iter 100. [Val Acc 58%, Loss 1.967889]\n",
            "Iter 10. [Val Acc 60%, Loss 1.551249]\n",
            "Iter 20. [Val Acc 59%, Loss 1.622027]\n",
            "Iter 30. [Val Acc 57%, Loss 2.121938]\n",
            "Iter 40. [Val Acc 60%, Loss 1.804808]\n",
            "Iter 50. [Val Acc 64%, Loss 1.354304]\n",
            "Iter 60. [Val Acc 66%, Loss 1.498683]\n",
            "Iter 70. [Val Acc 57%, Loss 1.815816]\n",
            "Iter 80. [Val Acc 61%, Loss 1.723323]\n",
            "Iter 90. [Val Acc 56%, Loss 2.320990]\n",
            "Iter 100. [Val Acc 70%, Loss 0.851920]\n",
            "Iter 10. [Val Acc 63%, Loss 1.305941]\n",
            "Iter 20. [Val Acc 63%, Loss 2.034575]\n",
            "Iter 30. [Val Acc 65%, Loss 1.462397]\n",
            "Iter 40. [Val Acc 68%, Loss 0.986941]\n",
            "Iter 50. [Val Acc 62%, Loss 1.713635]\n",
            "Iter 60. [Val Acc 66%, Loss 1.250979]\n",
            "Iter 70. [Val Acc 65%, Loss 1.746232]\n",
            "Iter 80. [Val Acc 59%, Loss 1.620685]\n",
            "Iter 90. [Val Acc 66%, Loss 1.113340]\n",
            "Iter 100. [Val Acc 62%, Loss 1.832639]\n",
            "Iter 10. [Val Acc 62%, Loss 1.454710]\n",
            "Iter 20. [Val Acc 62%, Loss 1.733275]\n",
            "Iter 30. [Val Acc 67%, Loss 1.127252]\n",
            "Iter 40. [Val Acc 62%, Loss 1.906576]\n",
            "Iter 50. [Val Acc 64%, Loss 1.370339]\n",
            "Iter 60. [Val Acc 63%, Loss 1.797683]\n",
            "Iter 70. [Val Acc 65%, Loss 1.241887]\n",
            "Iter 80. [Val Acc 66%, Loss 1.329240]\n",
            "Iter 90. [Val Acc 63%, Loss 1.858461]\n",
            "Iter 100. [Val Acc 66%, Loss 1.223198]\n",
            "Iter 10. [Val Acc 63%, Loss 1.625646]\n",
            "Iter 20. [Val Acc 64%, Loss 1.492554]\n",
            "Iter 30. [Val Acc 65%, Loss 1.192720]\n",
            "Iter 40. [Val Acc 63%, Loss 1.824272]\n",
            "Iter 50. [Val Acc 63%, Loss 1.628187]\n",
            "Iter 60. [Val Acc 65%, Loss 1.468329]\n",
            "Iter 70. [Val Acc 68%, Loss 1.130869]\n",
            "Iter 80. [Val Acc 64%, Loss 1.559571]\n",
            "Iter 90. [Val Acc 62%, Loss 2.175361]\n",
            "Iter 100. [Val Acc 63%, Loss 1.755383]\n",
            "Iter 10. [Val Acc 60%, Loss 1.913762]\n",
            "Iter 20. [Val Acc 63%, Loss 1.555336]\n",
            "Iter 30. [Val Acc 66%, Loss 1.237110]\n",
            "Iter 40. [Val Acc 67%, Loss 1.161284]\n",
            "Iter 50. [Val Acc 66%, Loss 1.335682]\n",
            "Iter 60. [Val Acc 65%, Loss 1.455932]\n",
            "Iter 70. [Val Acc 68%, Loss 1.031156]\n",
            "Iter 80. [Val Acc 65%, Loss 1.342605]\n",
            "Iter 90. [Val Acc 62%, Loss 2.258235]\n",
            "Iter 100. [Val Acc 66%, Loss 1.333543]\n",
            "Iter 10. [Val Acc 60%, Loss 2.053398]\n",
            "Iter 20. [Val Acc 65%, Loss 1.518135]\n",
            "Iter 30. [Val Acc 63%, Loss 1.587587]\n",
            "Iter 40. [Val Acc 65%, Loss 1.673512]\n",
            "Iter 50. [Val Acc 65%, Loss 1.535908]\n",
            "Iter 60. [Val Acc 66%, Loss 1.176266]\n",
            "Iter 70. [Val Acc 62%, Loss 2.055421]\n",
            "Iter 80. [Val Acc 66%, Loss 1.493541]\n",
            "Iter 90. [Val Acc 66%, Loss 1.442900]\n",
            "Iter 100. [Val Acc 64%, Loss 1.563806]\n",
            "Iter 10. [Val Acc 56%, Loss 2.630893]\n",
            "Iter 20. [Val Acc 62%, Loss 1.872707]\n",
            "Iter 30. [Val Acc 61%, Loss 2.362289]\n",
            "Iter 40. [Val Acc 57%, Loss 2.056478]\n",
            "Iter 50. [Val Acc 64%, Loss 2.316040]\n",
            "Iter 60. [Val Acc 60%, Loss 2.416468]\n",
            "Iter 70. [Val Acc 60%, Loss 2.316344]\n",
            "Iter 80. [Val Acc 67%, Loss 1.772493]\n",
            "Iter 90. [Val Acc 63%, Loss 2.226456]\n",
            "Iter 100. [Val Acc 61%, Loss 2.317869]\n",
            "Iter 10. [Val Acc 58%, Loss 2.666399]\n",
            "Iter 20. [Val Acc 65%, Loss 1.462081]\n",
            "Iter 30. [Val Acc 57%, Loss 2.263512]\n",
            "Iter 40. [Val Acc 60%, Loss 1.944946]\n",
            "Iter 50. [Val Acc 64%, Loss 1.511239]\n",
            "Iter 60. [Val Acc 58%, Loss 1.912474]\n",
            "Iter 70. [Val Acc 64%, Loss 1.807654]\n",
            "Iter 80. [Val Acc 60%, Loss 2.009081]\n",
            "Iter 90. [Val Acc 66%, Loss 1.810371]\n",
            "Iter 100. [Val Acc 68%, Loss 1.233129]\n",
            "Iter 10. [Val Acc 63%, Loss 1.632371]\n",
            "Iter 20. [Val Acc 66%, Loss 1.307021]\n",
            "Iter 30. [Val Acc 63%, Loss 2.077314]\n",
            "Iter 40. [Val Acc 61%, Loss 1.761652]\n",
            "Iter 50. [Val Acc 63%, Loss 1.650042]\n",
            "Iter 60. [Val Acc 65%, Loss 1.650803]\n",
            "Iter 70. [Val Acc 65%, Loss 1.504809]\n",
            "Iter 80. [Val Acc 53%, Loss 3.474735]\n",
            "Iter 90. [Val Acc 63%, Loss 1.557798]\n",
            "Iter 100. [Val Acc 69%, Loss 1.369228]\n",
            "Iter 10. [Val Acc 54%, Loss 2.467326]\n",
            "Iter 20. [Val Acc 57%, Loss 1.943811]\n",
            "Iter 30. [Val Acc 62%, Loss 1.580184]\n",
            "Iter 40. [Val Acc 60%, Loss 1.946868]\n",
            "Iter 50. [Val Acc 61%, Loss 1.773229]\n",
            "Iter 60. [Val Acc 65%, Loss 1.542863]\n",
            "Iter 70. [Val Acc 60%, Loss 1.784829]\n",
            "Iter 80. [Val Acc 64%, Loss 1.235923]\n",
            "Iter 90. [Val Acc 66%, Loss 1.342727]\n",
            "Iter 100. [Val Acc 60%, Loss 1.933397]\n",
            "Iter 10. [Val Acc 61%, Loss 1.710428]\n",
            "Iter 20. [Val Acc 61%, Loss 1.731774]\n",
            "Iter 30. [Val Acc 65%, Loss 1.407001]\n",
            "Iter 40. [Val Acc 62%, Loss 2.031259]\n",
            "Iter 50. [Val Acc 65%, Loss 1.698750]\n",
            "Iter 60. [Val Acc 68%, Loss 1.140210]\n",
            "Iter 70. [Val Acc 69%, Loss 1.098180]\n",
            "Iter 80. [Val Acc 66%, Loss 1.461620]\n",
            "Iter 90. [Val Acc 67%, Loss 1.178114]\n",
            "Iter 100. [Val Acc 62%, Loss 2.131150]\n",
            "Iter 10. [Val Acc 56%, Loss 2.559619]\n",
            "Iter 20. [Val Acc 57%, Loss 2.314610]\n",
            "Iter 30. [Val Acc 65%, Loss 1.501614]\n",
            "Iter 40. [Val Acc 57%, Loss 2.238558]\n",
            "Iter 50. [Val Acc 65%, Loss 1.396729]\n",
            "Iter 60. [Val Acc 60%, Loss 1.975663]\n",
            "Iter 70. [Val Acc 56%, Loss 2.543670]\n",
            "Iter 80. [Val Acc 58%, Loss 2.289083]\n",
            "Iter 90. [Val Acc 62%, Loss 1.725706]\n",
            "Iter 100. [Val Acc 69%, Loss 1.201445]\n",
            "Iter 10. [Val Acc 61%, Loss 1.941180]\n",
            "Iter 20. [Val Acc 62%, Loss 2.095488]\n",
            "Iter 30. [Val Acc 66%, Loss 1.161065]\n",
            "Iter 40. [Val Acc 64%, Loss 1.659319]\n",
            "Iter 50. [Val Acc 67%, Loss 1.321772]\n",
            "Iter 60. [Val Acc 67%, Loss 1.319470]\n",
            "Iter 70. [Val Acc 66%, Loss 1.544170]\n",
            "Iter 80. [Val Acc 64%, Loss 1.591271]\n",
            "Iter 90. [Val Acc 66%, Loss 1.480737]\n",
            "Iter 100. [Val Acc 60%, Loss 2.703817]\n",
            "Iter 10. [Val Acc 60%, Loss 2.153485]\n",
            "Iter 20. [Val Acc 63%, Loss 1.851771]\n",
            "Iter 30. [Val Acc 66%, Loss 1.382571]\n",
            "Iter 40. [Val Acc 67%, Loss 1.146400]\n",
            "Iter 50. [Val Acc 66%, Loss 1.435043]\n",
            "Iter 60. [Val Acc 63%, Loss 1.694107]\n",
            "Iter 70. [Val Acc 68%, Loss 1.333739]\n",
            "Iter 80. [Val Acc 64%, Loss 1.920442]\n",
            "Iter 90. [Val Acc 60%, Loss 2.253313]\n",
            "Iter 100. [Val Acc 62%, Loss 2.255902]\n",
            "Iter 10. [Val Acc 60%, Loss 2.244949]\n",
            "Iter 20. [Val Acc 61%, Loss 2.317835]\n",
            "Iter 30. [Val Acc 62%, Loss 1.862459]\n",
            "Iter 40. [Val Acc 68%, Loss 1.078444]\n",
            "Iter 50. [Val Acc 60%, Loss 2.774225]\n",
            "Iter 60. [Val Acc 64%, Loss 1.775109]\n",
            "Iter 70. [Val Acc 66%, Loss 1.399744]\n",
            "Iter 80. [Val Acc 64%, Loss 1.829154]\n",
            "Iter 90. [Val Acc 67%, Loss 1.283809]\n",
            "Iter 100. [Val Acc 67%, Loss 1.590582]\n",
            "Iter 10. [Val Acc 59%, Loss 2.324388]\n",
            "Iter 20. [Val Acc 60%, Loss 2.493409]\n",
            "Iter 30. [Val Acc 64%, Loss 1.888875]\n",
            "Iter 40. [Val Acc 54%, Loss 3.871105]\n",
            "Iter 50. [Val Acc 57%, Loss 2.865038]\n",
            "Iter 60. [Val Acc 65%, Loss 2.127273]\n",
            "Iter 70. [Val Acc 65%, Loss 2.012455]\n",
            "Iter 80. [Val Acc 64%, Loss 1.808000]\n",
            "Iter 90. [Val Acc 63%, Loss 1.839509]\n",
            "Iter 100. [Val Acc 64%, Loss 1.944139]\n",
            "Iter 10. [Val Acc 60%, Loss 2.026878]\n",
            "Iter 20. [Val Acc 58%, Loss 2.198756]\n",
            "Iter 30. [Val Acc 63%, Loss 1.633826]\n",
            "Iter 40. [Val Acc 66%, Loss 1.752981]\n",
            "Iter 50. [Val Acc 60%, Loss 3.011375]\n",
            "Iter 60. [Val Acc 67%, Loss 1.375798]\n",
            "Iter 70. [Val Acc 64%, Loss 1.247842]\n",
            "Iter 80. [Val Acc 63%, Loss 1.836901]\n",
            "Iter 90. [Val Acc 65%, Loss 1.433320]\n",
            "Iter 100. [Val Acc 54%, Loss 3.049399]\n",
            "Iter 10. [Val Acc 63%, Loss 1.830199]\n",
            "Iter 20. [Val Acc 58%, Loss 3.287076]\n",
            "Iter 30. [Val Acc 66%, Loss 1.329586]\n",
            "Iter 40. [Val Acc 67%, Loss 1.277163]\n",
            "Iter 50. [Val Acc 56%, Loss 2.611995]\n",
            "Iter 60. [Val Acc 60%, Loss 1.935871]\n",
            "Iter 70. [Val Acc 60%, Loss 1.852909]\n",
            "Iter 80. [Val Acc 67%, Loss 1.304938]\n",
            "Iter 90. [Val Acc 62%, Loss 1.572077]\n",
            "Iter 100. [Val Acc 60%, Loss 1.658390]\n",
            "Iter 10. [Val Acc 60%, Loss 1.881063]\n",
            "Iter 20. [Val Acc 65%, Loss 1.499877]\n",
            "Iter 30. [Val Acc 68%, Loss 1.291863]\n",
            "Iter 40. [Val Acc 66%, Loss 1.736840]\n",
            "Iter 50. [Val Acc 64%, Loss 1.930515]\n",
            "Iter 60. [Val Acc 62%, Loss 2.787112]\n",
            "Iter 70. [Val Acc 64%, Loss 1.861801]\n",
            "Iter 80. [Val Acc 62%, Loss 2.089966]\n",
            "Iter 90. [Val Acc 64%, Loss 1.933688]\n",
            "Iter 100. [Val Acc 63%, Loss 1.704130]\n",
            "Iter 10. [Val Acc 60%, Loss 2.618086]\n",
            "Iter 20. [Val Acc 61%, Loss 2.803313]\n",
            "Iter 30. [Val Acc 62%, Loss 2.385514]\n",
            "Iter 40. [Val Acc 65%, Loss 1.601549]\n",
            "Iter 50. [Val Acc 65%, Loss 1.651235]\n",
            "Iter 60. [Val Acc 65%, Loss 1.848344]\n",
            "Iter 70. [Val Acc 64%, Loss 1.607631]\n",
            "Iter 80. [Val Acc 64%, Loss 1.442637]\n",
            "Iter 90. [Val Acc 65%, Loss 1.319679]\n",
            "Iter 100. [Val Acc 63%, Loss 1.647498]\n",
            "Iter 10. [Val Acc 62%, Loss 1.548657]\n",
            "Iter 20. [Val Acc 63%, Loss 1.403005]\n",
            "Iter 30. [Val Acc 64%, Loss 1.894942]\n",
            "Iter 40. [Val Acc 65%, Loss 1.970888]\n",
            "Iter 50. [Val Acc 64%, Loss 1.849708]\n",
            "Iter 60. [Val Acc 64%, Loss 1.877475]\n",
            "Iter 70. [Val Acc 63%, Loss 2.275658]\n",
            "Iter 80. [Val Acc 65%, Loss 1.883359]\n",
            "Iter 90. [Val Acc 64%, Loss 2.156630]\n",
            "Iter 100. [Val Acc 62%, Loss 2.640113]\n",
            "Iter 10. [Val Acc 60%, Loss 2.679119]\n",
            "Iter 20. [Val Acc 64%, Loss 1.693106]\n",
            "Iter 30. [Val Acc 63%, Loss 2.142823]\n",
            "Iter 40. [Val Acc 65%, Loss 1.907948]\n",
            "Iter 50. [Val Acc 64%, Loss 1.903078]\n",
            "Iter 60. [Val Acc 68%, Loss 1.071824]\n",
            "Iter 70. [Val Acc 67%, Loss 1.495602]\n",
            "Iter 80. [Val Acc 70%, Loss 1.107551]\n",
            "Iter 90. [Val Acc 61%, Loss 2.307493]\n",
            "Iter 100. [Val Acc 62%, Loss 2.503424]\n",
            "Iter 10. [Val Acc 61%, Loss 2.454489]\n",
            "Iter 20. [Val Acc 63%, Loss 1.838307]\n",
            "Iter 30. [Val Acc 61%, Loss 2.323269]\n",
            "Iter 40. [Val Acc 63%, Loss 2.030333]\n",
            "Iter 50. [Val Acc 64%, Loss 2.243882]\n",
            "Iter 60. [Val Acc 66%, Loss 1.750571]\n",
            "Iter 70. [Val Acc 63%, Loss 2.046079]\n",
            "Iter 80. [Val Acc 65%, Loss 1.874734]\n",
            "Iter 90. [Val Acc 63%, Loss 1.926185]\n",
            "Iter 100. [Val Acc 64%, Loss 2.004774]\n",
            "Iter 10. [Val Acc 55%, Loss 2.532091]\n",
            "Iter 20. [Val Acc 60%, Loss 1.791742]\n",
            "Iter 30. [Val Acc 58%, Loss 2.237242]\n",
            "Iter 40. [Val Acc 59%, Loss 2.225388]\n",
            "Iter 50. [Val Acc 59%, Loss 2.202594]\n",
            "Iter 60. [Val Acc 58%, Loss 2.368354]\n",
            "Iter 70. [Val Acc 60%, Loss 2.317331]\n",
            "Iter 80. [Val Acc 65%, Loss 1.589296]\n",
            "Iter 90. [Val Acc 61%, Loss 2.046612]\n",
            "Iter 100. [Val Acc 62%, Loss 1.997618]\n",
            "optimal cost =  0.6036628520616335 , optimal accuracy =  0.707 , optimal mu =  1.0000000000000002 , optimal batch =  450\n",
            "optimal b =  0.26591463312083924 , optimal w =  [ 1.38099426e+00 -9.55088924e-01 -1.33487977e-01 -3.01403344e-01\n",
            " -4.69951482e-02 -4.47709632e-01  7.24503189e-02 -1.60988121e-01\n",
            " -2.37485244e-01  1.85158045e-01 -9.04714880e-02  4.37933204e-02\n",
            "  3.46394911e-01  1.53022301e-01 -9.14906374e-02  3.06724134e-01\n",
            "  1.02418038e-01  5.50608855e-01  3.69645587e-01  5.58112843e-01\n",
            "  1.00523481e-01 -3.21835317e-01 -4.07804224e-01  3.12272228e-01\n",
            " -1.56489345e-01 -2.24245354e-02  2.64121274e-01 -6.41420251e-02\n",
            "  6.90161324e-02  1.25398419e-01  8.95145564e-04  6.17037578e-02\n",
            " -1.40005901e-01  1.31011840e-01 -1.82824251e-01 -8.81727146e-02\n",
            "  3.86369597e-03  3.76650445e-03  2.87199860e-02  2.44821356e-02\n",
            " -1.19350468e-01  3.84562163e-02  5.62110714e-02  6.26104994e-02\n",
            "  8.78601465e-02  6.50044922e-03  8.58272658e-02 -1.88751200e-01\n",
            "  1.81754107e-02  1.07706828e-01  7.25273233e-02  9.36399029e-02\n",
            "  8.40114285e-02 -1.82726955e-01 -7.73175101e-02 -3.41852517e-02\n",
            " -1.79147117e-01  1.16175919e-01  2.77981756e-02 -5.68969133e-02\n",
            " -2.52450952e-02 -1.70140274e-02 -9.91897388e-02  6.39313823e-02\n",
            " -1.47937135e-01 -4.84100116e-04  3.62044556e-02 -6.69818744e-02\n",
            " -1.66245460e-01 -4.38756695e-02 -5.81122693e-02  5.96872559e-02\n",
            "  4.44306735e-02  1.17216084e-01 -2.10301643e-02  5.00069546e-02\n",
            "  1.34907202e-02 -1.18247589e-01 -2.12963095e-01 -1.00885184e-01\n",
            " -6.45365293e-04 -4.99635145e-02 -4.26320109e-02  9.29038432e-02\n",
            "  3.22308000e-03 -1.40939776e-02  1.25432803e-01 -1.94839127e-01\n",
            " -9.70756141e-02  1.32580460e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pcolormesh(np.arange(batch_start, batch_end+batch_res, batch_res), np.arange(mu_start, mu_end+mu_res, mu_res),acc_mat)\n",
        "plt.title(\"Accuracy as Function of Learning Rate & Batch Size\")\n",
        "plt.xlabel(\"Batch Size\")\n",
        "plt.ylabel(\"Î¼\")\n",
        "plt.colorbar();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hHZ2GQj0H32y",
        "outputId": "3306b341-eb54-4ad1-8647-222f0ca8fb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZt7_932zX2"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the optimals mu and batch size are between 1-2 and 300-500 respectivly.\n",
        "\n",
        "We can see another good result at  mu=4 and batch size=250, it is an expremental result and could be a coincident."
      ],
      "metadata": {
        "id": "X5RGyvBT72Wr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### Part (h) -- 15%\n",
        "\n",
        "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKw2mLozGrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569c0ecc-de61-4d93-93b8-e8a24d75e337"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "train_acc = get_accuracy(pred(w_opt,b_opt,train_norm_xs), train_ts)*100\n",
        "val_acc = get_accuracy(pred(w_opt,b_opt,val_norm_xs), val_ts)*100\n",
        "test_acc = get_accuracy(pred(w_opt,b_opt,test_norm_xs), test_ts)*100\n",
        "\n",
        "print('train_acc = {:.4f}%\\t val_acc = {:.4f}%\\t test_acc = {:.4f}%'.format(train_acc, val_acc, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc = 71.5746%\t val_acc = 71.0040%\t test_acc = 70.9801%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZa1u6920M3"
      },
      "source": [
        "**Explain and discuss your results here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the train has the highest accuracy because the model fitts the weights according to the train data.\n",
        "The validatin data is used to find the optimal hyperparameters which affects the model.\n",
        "The test is the only data that the model have not seen before and did not adjust to it.\n",
        "\n",
        "The test accuracy is very close to the train and validation accuracy which shows that the model is robust and dosn't overfit to the training and validation data."
      ],
      "metadata": {
        "id": "DIXQr6xl9ToB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4eP2Yh1zGrI"
      },
      "source": [
        "### Part (i) -- 15%\n",
        "\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
        "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "Compute the training, validation and test accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LCfAa1zGrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6672e21-3cb8-4880-81f7-040cce14f0e4"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(random_state=0).fit(train_norm_xs, np.squeeze(train_ts))\n",
        "\n",
        "# In order to achieve the model accuracy, we can use either the exsisted function `get_accuracy` or method model.score\n",
        "\n",
        "# Using exsisted function `get_accuracy`\n",
        "train_acc = get_accuracy(model.predict(train_norm_xs), train_ts)*100\n",
        "val_acc = get_accuracy(model.predict(val_norm_xs), val_ts)*100\n",
        "test_acc = get_accuracy(model.predict(test_norm_xs), test_ts)*100\n",
        "\n",
        "# Using method model.score\n",
        "train_acc1 = model.score(train_norm_xs, train_ts)*100\n",
        "val_acc1 = model.score(val_norm_xs, val_ts)*100\n",
        "test_acc1 = model.score(test_norm_xs, test_ts)*100\n",
        "\n",
        "print('Using get_accuracy:\\ttrain_acc = {:.4f}%\\t val_acc = {:.4f}%\\t test_acc = {:.4f}%'.format(train_acc, val_acc, test_acc))\n",
        "print('Using model.score:\\ttrain_acc = {:.4f}%\\t val_acc = {:.4f}%\\t test_acc = {:.4f}%'.format(train_acc1, val_acc1, test_acc1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using get_accuracy:\ttrain_acc = 73.2942%\t val_acc = 73.3540%\t test_acc = 72.6980%\n",
            "Using model.score:\ttrain_acc = 73.2942%\t val_acc = 73.3540%\t test_acc = 72.6980%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqucdV923tG"
      },
      "source": [
        "**This parts helps by checking if the code worked.**\n",
        "**Check if you get similar results, if not repair your code**\n"
      ]
    }
  ]
}